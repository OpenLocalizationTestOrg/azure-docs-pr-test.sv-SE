---
title: "aaaAzure Service Fabric-katastrofåterställning | Microsoft Docs"
description: "Azure Service Fabric erbjuder hello funktioner nödvändiga toodeal med alla typer av katastrofer. Den här artikeln beskriver hello typer av katastrofer som kan uppstå och hur toodeal med dem.."
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: ab49c4b9-74a8-4907-b75b-8d2ee84c6d90
ms.service: service-fabric
ms.devlang: dotNet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 04b8348fb63e8a1c76a8f722c4c8255b339908e2
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: sv-SE
ms.lasthandoff: 10/06/2017
---
# <a name="disaster-recovery-in-azure-service-fabric"></a><span data-ttu-id="900c1-104">Katastrofåterställning i Azure Service Fabric</span><span class="sxs-lookup"><span data-stu-id="900c1-104">Disaster recovery in Azure Service Fabric</span></span>
<span data-ttu-id="900c1-105">En viktig del av ger hög tillgänglighet är att säkerställa att tjänster kan överleva alla olika typer av fel.</span><span class="sxs-lookup"><span data-stu-id="900c1-105">A critical part of delivering high-availability is ensuring that services can survive all different types of failures.</span></span> <span data-ttu-id="900c1-106">Detta är särskilt viktigt för fel som är oplanerade och utanför din kontroll.</span><span class="sxs-lookup"><span data-stu-id="900c1-106">This is especially important for failures that are unplanned and outside of your control.</span></span> <span data-ttu-id="900c1-107">Den här artikeln beskriver vissa vanliga feltillstånd som kan vara katastrofer om inte modelleras och hanteras korrekt.</span><span class="sxs-lookup"><span data-stu-id="900c1-107">This article describes some common failure modes that could be disasters if not modeled and managed correctly.</span></span> <span data-ttu-id="900c1-108">Åtgärder och åtgärder tootake beskrivs även om en katastrof hände ändå.</span><span class="sxs-lookup"><span data-stu-id="900c1-108">It also discuss mitigations and actions tootake if a disaster happened anyway.</span></span> <span data-ttu-id="900c1-109">hello målet är toolimit eller eliminera hello risken för avbrott eller dataförlust när de uppstår fel, planerade eller annars kan uppstå.</span><span class="sxs-lookup"><span data-stu-id="900c1-109">hello goal is toolimit or eliminate hello risk of downtime or data loss when they occur failures, planned or otherwise, occur.</span></span>

## <a name="avoiding-disaster"></a><span data-ttu-id="900c1-110">Undvika katastrofåterställning</span><span class="sxs-lookup"><span data-stu-id="900c1-110">Avoiding disaster</span></span>
<span data-ttu-id="900c1-111">Service Fabric primära målet är toohelp du modellen både din miljö och dina tjänster så att den vanliga fel typer inte är katastrofer.</span><span class="sxs-lookup"><span data-stu-id="900c1-111">Service Fabric's primary goal is toohelp you model both your environment and your services in such a way that common failure types are not disasters.</span></span> 

<span data-ttu-id="900c1-112">I allmänhet finns två typer av disaster/scenarier:</span><span class="sxs-lookup"><span data-stu-id="900c1-112">In general there are two types of disaster/failure scenarios:</span></span>

1. <span data-ttu-id="900c1-113">Maskinvaru- eller fel</span><span class="sxs-lookup"><span data-stu-id="900c1-113">Hardware or software faults</span></span>
2. <span data-ttu-id="900c1-114">Använd fel</span><span class="sxs-lookup"><span data-stu-id="900c1-114">Operational faults</span></span>

### <a name="hardware-and-software-faults"></a><span data-ttu-id="900c1-115">Fel för maskinvara och programvara</span><span class="sxs-lookup"><span data-stu-id="900c1-115">Hardware and software faults</span></span>
<span data-ttu-id="900c1-116">Fel för maskinvara och programvara är oförutsägbart.</span><span class="sxs-lookup"><span data-stu-id="900c1-116">Hardware and software faults are unpredictable.</span></span> <span data-ttu-id="900c1-117">hello enklaste sättet toosurvive fel körs fler kopior av hello service omfattas över maskinvaru- eller fault-gränser.</span><span class="sxs-lookup"><span data-stu-id="900c1-117">hello easiest way toosurvive faults is running more copies of hello service  spanned across hardware or software fault boundaries.</span></span> <span data-ttu-id="900c1-118">Till exempel om din tjänst körs bara på en viss dator, är sedan hello felet på att en dator en katastrofåterställning för tjänsten.</span><span class="sxs-lookup"><span data-stu-id="900c1-118">For example, if your service is running only on one particular machine, then hello failure of that one machine is a disaster for that service.</span></span> <span data-ttu-id="900c1-119">hello enkelt tooavoid denna katastrofåterställning är tooensure hello faktiskt körs på flera datorer.</span><span class="sxs-lookup"><span data-stu-id="900c1-119">hello simple way tooavoid this disaster is tooensure that hello service is actually running on multiple machines.</span></span> <span data-ttu-id="900c1-120">Testning är också nödvändigt tooensure hello fel på en dator inte störa hello tjänsten körs.</span><span class="sxs-lookup"><span data-stu-id="900c1-120">Testing is also necessary tooensure hello failure of one machine doesn't disrupt hello running service.</span></span> <span data-ttu-id="900c1-121">Kapacitetsplanering säkerställer en ersättning instans kan skapas på en annan plats och att minskad kapacitet inte överlagra hello återstående tjänster.</span><span class="sxs-lookup"><span data-stu-id="900c1-121">Capacity planning ensures a replacement instance can be created elsewhere and that reduction in capacity doesn't overload hello remaining services.</span></span> <span data-ttu-id="900c1-122">hello samma mönster fungerar oavsett vad du skulle tooavoid hello fel i.</span><span class="sxs-lookup"><span data-stu-id="900c1-122">hello same pattern works regardless of what you're trying tooavoid hello failure of.</span></span> <span data-ttu-id="900c1-123">Till exempel.</span><span class="sxs-lookup"><span data-stu-id="900c1-123">For example.</span></span> <span data-ttu-id="900c1-124">Om du är orolig hello fel i ett SAN-nätverk kan köra du över flera SAN-nätverk.</span><span class="sxs-lookup"><span data-stu-id="900c1-124">if you're concerned about hello failure of a SAN, you run across multiple SANs.</span></span> <span data-ttu-id="900c1-125">Om du vill göra hello förlust av en samling servrar, kör över flera rack.</span><span class="sxs-lookup"><span data-stu-id="900c1-125">If you're concerned about hello loss of a rack of servers, you run across multiple racks.</span></span> <span data-ttu-id="900c1-126">Om du oroar hello förlust av datacenter, ska tjänsten köras i flera Azure-regioner eller Datacenter.</span><span class="sxs-lookup"><span data-stu-id="900c1-126">If you're worried about hello loss of datacenters, your service should run across multiple Azure regions or datacenters.</span></span> 

<span data-ttu-id="900c1-127">När den körs i den här typen av disklänkande läge du fortfarande är toosome ämnestyper för samtidiga fel, men enskilt och även flera fel i en viss typ (ex: en enskild virtuell dator eller nätverket länken misslyckas) hanteras automatiskt (och därför inte längre en ”katastrof”).</span><span class="sxs-lookup"><span data-stu-id="900c1-127">When running in this type of spanned mode, you're still subject toosome types of simultaneous failures, but single and even multiple failures of a particular type (ex: a single VM or network link failing) are automatically handled (and so no longer a "disaster").</span></span> <span data-ttu-id="900c1-128">Service Fabric innehåller många metoder för att expandera hello klustret och hanterar tillbaka när felande noder och tjänster.</span><span class="sxs-lookup"><span data-stu-id="900c1-128">Service Fabric provides many mechanisms for expanding hello cluster and handles bringing failed nodes and services back.</span></span> <span data-ttu-id="900c1-129">Service Fabric kan också kör flera instanser av tjänsterna i ordning tooavoid dessa typer av oplanerade fel från aktivera i riktiga katastrofer.</span><span class="sxs-lookup"><span data-stu-id="900c1-129">Service Fabric also allows running many instances of your services in order tooavoid these types of unplanned failures from turning into real disasters.</span></span>

<span data-ttu-id="900c1-130">Det kan finnas skäl varför kör en tillräckligt stor toospan distribution över fel inte är möjligt.</span><span class="sxs-lookup"><span data-stu-id="900c1-130">There may be reasons why running a deployment large enough toospan over failures is not feasible.</span></span> <span data-ttu-id="900c1-131">Det kan till exempel ta mer maskinvaruresurser än du inte vill toopay för relativa toohello risken för fel.</span><span class="sxs-lookup"><span data-stu-id="900c1-131">For example, it may take more hardware resources than you're not willing toopay for relative toohello chance of failure.</span></span> <span data-ttu-id="900c1-132">När du hanterar distribuerade program, kan det bero på att ytterligare kommunikation hopp eller tillstånd replikering kostnader över geografiska avstånd orsaker acceptabel svarstid.</span><span class="sxs-lookup"><span data-stu-id="900c1-132">When dealing with distributed applications, it could be that additional communication hops or state replication costs across geographic distances causes unacceptable latency.</span></span> <span data-ttu-id="900c1-133">Om den här raden ritas skiljer sig för varje program.</span><span class="sxs-lookup"><span data-stu-id="900c1-133">Where this line is drawn differs for each application.</span></span> <span data-ttu-id="900c1-134">För programvarufel specifikt kan hello fel vara i hello-tjänsten som du försöker tooscale.</span><span class="sxs-lookup"><span data-stu-id="900c1-134">For software faults specifically, hello fault could be in hello service that you are trying tooscale.</span></span> <span data-ttu-id="900c1-135">I det här fallet förhindra inte fler kopior hello katastrofåterställning, eftersom hello feltillståndet korreleras över alla hello-instanser.</span><span class="sxs-lookup"><span data-stu-id="900c1-135">In this case more copies don't prevent hello disaster, since hello failure condition is correlated across all hello instances.</span></span>

### <a name="operational-faults"></a><span data-ttu-id="900c1-136">Använd fel</span><span class="sxs-lookup"><span data-stu-id="900c1-136">Operational faults</span></span>
<span data-ttu-id="900c1-137">Även om tjänsten omfattas över hello jordglob med många uppsägningar, kan det fortfarande har katastrofala händelser.</span><span class="sxs-lookup"><span data-stu-id="900c1-137">Even if your service is spanned across hello globe with many redundancies, it can still experience disastrous events.</span></span> <span data-ttu-id="900c1-138">Till exempel om någon av misstag konfigurerar hello dns-namn för tjänsten hello, eller tar bort den direkt.</span><span class="sxs-lookup"><span data-stu-id="900c1-138">For example, if someone accidentally reconfigures hello dns name for hello service, or deletes it outright.</span></span> <span data-ttu-id="900c1-139">Exempel anta att du har en tillståndskänslig Service Fabric-tjänst och någon av misstag tas bort tjänsten.</span><span class="sxs-lookup"><span data-stu-id="900c1-139">As an example, let's say you had a stateful Service Fabric service, and someone deleted that service accidentally.</span></span> <span data-ttu-id="900c1-140">Om det inte finns några andra minskning, tjänsten och alla hello tillstånd den hade är nu rest.</span><span class="sxs-lookup"><span data-stu-id="900c1-140">Unless there's some other mitigation, that service and all of hello state it had is now gone.</span></span> <span data-ttu-id="900c1-141">Dessa typer av operativa katastrofer kräver (”hoppsan”) olika åtgärder och steg för återställning än vanliga oplanerade fel.</span><span class="sxs-lookup"><span data-stu-id="900c1-141">These types of operational disasters ("oops") require different mitigations and steps for recovery than regular unplanned failures.</span></span> 

<span data-ttu-id="900c1-142">hello bästa sätt tooavoid dessa typer av operativa fel är att</span><span class="sxs-lookup"><span data-stu-id="900c1-142">hello best ways tooavoid these types of operational faults are to</span></span>
1. <span data-ttu-id="900c1-143">begränsa operativa åtkomst toohello miljö</span><span class="sxs-lookup"><span data-stu-id="900c1-143">restrict operational access toohello environment</span></span>
2. <span data-ttu-id="900c1-144">strikt granskningsåtgärder farliga</span><span class="sxs-lookup"><span data-stu-id="900c1-144">strictly audit dangerous operations</span></span>
3. <span data-ttu-id="900c1-145">införa automation, förhindra manuell eller out-of-band-ändringar och verifiera specifika ändringar mot hello faktiska miljö innan anta dem</span><span class="sxs-lookup"><span data-stu-id="900c1-145">impose automation, prevent manual or out of band changes, and validate specific changes against hello actual environment before enacting them</span></span>
4. <span data-ttu-id="900c1-146">Se till att skadliga åtgärder ”soft”.</span><span class="sxs-lookup"><span data-stu-id="900c1-146">ensure that destructive operations are "soft".</span></span> <span data-ttu-id="900c1-147">Mjuka operations börjar inte gälla omedelbart eller kan ångras i vissa tidsfönstret</span><span class="sxs-lookup"><span data-stu-id="900c1-147">Soft operations don't take effect immediately or can be undone within some time window</span></span>

<span data-ttu-id="900c1-148">Service Fabric innehåller vissa mekanismer tooprevent operativa fel, till exempel att tillhandahålla [rollbaserad](service-fabric-cluster-security-roles.md) åtkomstkontroll för klusteråtgärder för.</span><span class="sxs-lookup"><span data-stu-id="900c1-148">Service Fabric provides some mechanisms tooprevent operational faults, such as providing [role-based](service-fabric-cluster-security-roles.md) access control for cluster operations.</span></span> <span data-ttu-id="900c1-149">De flesta av dessa operativa fel kräver dock arbete och andra system.</span><span class="sxs-lookup"><span data-stu-id="900c1-149">However, most of these operational faults require organizational efforts and other systems.</span></span> <span data-ttu-id="900c1-150">Service Fabric tillhandahåller en mekanism för kvarvarande operativa fel, framför allt säkerhetskopiering och återställning för tillståndskänsliga tjänster.</span><span class="sxs-lookup"><span data-stu-id="900c1-150">Service Fabric does provide some mechanism for surviving operational faults, most notably backup and restore for stateful services.</span></span>

## <a name="managing-failures"></a><span data-ttu-id="900c1-151">Hantera fel</span><span class="sxs-lookup"><span data-stu-id="900c1-151">Managing failures</span></span>
<span data-ttu-id="900c1-152">hello målet för Service Fabric är nästan alltid automatisk hantering av fel.</span><span class="sxs-lookup"><span data-stu-id="900c1-152">hello goal of Service Fabric is almost always automatic management of failures.</span></span> <span data-ttu-id="900c1-153">Men i order toohandle vissa typer av fel, tjänster måste ha ytterligare kod.</span><span class="sxs-lookup"><span data-stu-id="900c1-153">However, in order toohandle some types of failures, services must have additional code.</span></span> <span data-ttu-id="900c1-154">Andra typer av fel bör _inte_ åtgärdas automatiskt av säkerhet och business continuity skäl.</span><span class="sxs-lookup"><span data-stu-id="900c1-154">Other types of failures should _not_ be automatically addressed because of safety and business continuity reasons.</span></span> 

### <a name="handling-single-failures"></a><span data-ttu-id="900c1-155">Enkel hantering-fel</span><span class="sxs-lookup"><span data-stu-id="900c1-155">Handling single failures</span></span>
<span data-ttu-id="900c1-156">Enskild datorerna kan växlas till en mängd olika skäl.</span><span class="sxs-lookup"><span data-stu-id="900c1-156">Single machines can fail for all sorts of reasons.</span></span> <span data-ttu-id="900c1-157">Vissa av dessa är maskinvara orsaker som strömkällor och nätverk maskinvarufel.</span><span class="sxs-lookup"><span data-stu-id="900c1-157">Some of these are hardware causes, like power supplies and networking hardware failures.</span></span> <span data-ttu-id="900c1-158">Andra fel finns i programvaran.</span><span class="sxs-lookup"><span data-stu-id="900c1-158">Other failures are in software.</span></span> <span data-ttu-id="900c1-159">Dessa inkluderar fel hello faktiska operativsystem och själva hello-tjänsten.</span><span class="sxs-lookup"><span data-stu-id="900c1-159">These include failures of hello actual operating system and hello service itself.</span></span> <span data-ttu-id="900c1-160">Service Fabric identifierar automatiskt dessa typer av fel, inklusive fall där hello datorn blir isolerad från andra datorer på grund av problem med toonetwork.</span><span class="sxs-lookup"><span data-stu-id="900c1-160">Service Fabric automatically detects these types of failures, including cases where hello machine becomes isolated from other machines due toonetwork issues.</span></span>

<span data-ttu-id="900c1-161">Oavsett hello typ av tjänst, kör en enda instans resultat driftstopp för tjänsten om den enda kopian av hello kod misslyckas av någon anledning.</span><span class="sxs-lookup"><span data-stu-id="900c1-161">Regardless of hello type of service, running a single instance results in downtime for that service if that single copy of hello code fails for any reason.</span></span> 

<span data-ttu-id="900c1-162">I ordning toohandle varje enskilt fel hello enklaste som du kan göra är tooensure som dina tjänster som körs på fler än en nod som standard.</span><span class="sxs-lookup"><span data-stu-id="900c1-162">In order toohandle any single failure, hello simplest thing you can do is tooensure that your services run on more than one node by default.</span></span> <span data-ttu-id="900c1-163">För tillståndslösa tjänster detta kan åstadkommas genom att låta en `InstanceCount` större än 1.</span><span class="sxs-lookup"><span data-stu-id="900c1-163">For stateless services, this can be accomplished by having an `InstanceCount` greater than 1.</span></span> <span data-ttu-id="900c1-164">För tillståndskänsliga tjänster hello minsta rekommendation är alltid en `TargetReplicaSetSize` och `MinReplicaSetSize` på minst 3.</span><span class="sxs-lookup"><span data-stu-id="900c1-164">For stateful services, hello minimum recommendation is always a `TargetReplicaSetSize` and `MinReplicaSetSize` of at least 3.</span></span> <span data-ttu-id="900c1-165">Kör fler kopior av koden för tjänsten säkerställer att din tjänst kan hantera varje enskilt fel automatiskt.</span><span class="sxs-lookup"><span data-stu-id="900c1-165">Running more copies of your service code ensures that your service can handle any single failure automatically.</span></span> 

### <a name="handling-coordinated-failures"></a><span data-ttu-id="900c1-166">Hantering av koordineras fel</span><span class="sxs-lookup"><span data-stu-id="900c1-166">Handling coordinated failures</span></span>
<span data-ttu-id="900c1-167">Samordnade fel kan inträffa i ett kluster på grund av tooeither planerad eller oplanerad infrastruktur fel och ändringar eller planerade programändringar.</span><span class="sxs-lookup"><span data-stu-id="900c1-167">Coordinated failures can happen in a cluster due tooeither planned or unplanned infrastructure failures and changes, or planned software changes.</span></span> <span data-ttu-id="900c1-168">Service Fabric modeller infrastruktur zoner som samordnade fel som Feldomäner uppstår.</span><span class="sxs-lookup"><span data-stu-id="900c1-168">Service Fabric models infrastructure zones that experience coordinated failures as Fault Domains.</span></span> <span data-ttu-id="900c1-169">Områden som får samordnade programvaruändringar modelleras som uppgradera domäner.</span><span class="sxs-lookup"><span data-stu-id="900c1-169">Areas that will experience coordinated software changes are modeled as Upgrade Domains.</span></span> <span data-ttu-id="900c1-170">Mer information om fel- och domäner finns i [dokumentet](service-fabric-cluster-resource-manager-cluster-description.md) som beskriver klustertopologi och definition.</span><span class="sxs-lookup"><span data-stu-id="900c1-170">More information about fault and upgrade domains is in [this document](service-fabric-cluster-resource-manager-cluster-description.md) that describes cluster topology and definition.</span></span>

<span data-ttu-id="900c1-171">Som standard anser Service Fabric-fel och uppgradera domäner när du planerar där dina tjänster ska köras.</span><span class="sxs-lookup"><span data-stu-id="900c1-171">By default Service Fabric considers fault and upgrade domains when planning where your services should run.</span></span> <span data-ttu-id="900c1-172">Som standard försöker Service Fabric tooensure som dina tjänster körs över flera fel- och domäner så om planerad eller oplanerad ändringar sker tjänsterna är tillgängliga.</span><span class="sxs-lookup"><span data-stu-id="900c1-172">By default, Service Fabric tries tooensure that your services run across several fault and upgrade domains so if planned or unplanned changes happen your services remain available.</span></span> 

<span data-ttu-id="900c1-173">Anta exempelvis att fel i ett eluttag orsakar en samling datorer toofail samtidigt.</span><span class="sxs-lookup"><span data-stu-id="900c1-173">For example, let's say that failure of a power source causes a rack of machines toofail simultaneously.</span></span> <span data-ttu-id="900c1-174">Blir ett exempel på en enda misslyckades för en viss tjänst med flera kopior av hello-tjänsten körs hello förlust av många datorer i fel domän fel.</span><span class="sxs-lookup"><span data-stu-id="900c1-174">With multiple copies of hello service running hello loss of many machines in fault domain failure turns into just another example of single failure for a given service.</span></span> <span data-ttu-id="900c1-175">Det är därför hantera feldomäner är kritiska tooensuring hög tillgänglighet för dina tjänster.</span><span class="sxs-lookup"><span data-stu-id="900c1-175">This is why managing fault domains is critical tooensuring high availability of your services.</span></span> <span data-ttu-id="900c1-176">När du kör Service Fabric i Azure, hanteras automatiskt feldomäner.</span><span class="sxs-lookup"><span data-stu-id="900c1-176">When running Service Fabric in Azure, fault domains are managed automatically.</span></span> <span data-ttu-id="900c1-177">I andra miljöer kanske de inte.</span><span class="sxs-lookup"><span data-stu-id="900c1-177">In other environments they may not be.</span></span> <span data-ttu-id="900c1-178">Om du utvecklar dina egna kluster lokalt vara säker på att toomap och planera fel domän layouten på rätt sätt.</span><span class="sxs-lookup"><span data-stu-id="900c1-178">If you're building your own clusters on premises, be sure toomap and plan your fault domain layout correctly.</span></span>

<span data-ttu-id="900c1-179">Uppgraderingsdomäner är användbara för att modellera områden där programvaran kommer toobe uppgraderas vid hello samma tid.</span><span class="sxs-lookup"><span data-stu-id="900c1-179">Upgrade Domains are useful for modeling areas where software is going toobe upgraded at hello same time.</span></span> <span data-ttu-id="900c1-180">Därför definiera uppgradera domäner också ofta hello gränser där programvara tas under planerade uppgraderingar.</span><span class="sxs-lookup"><span data-stu-id="900c1-180">Because of this, Upgrade Domains also often define hello boundaries where software is taken down during planned upgrades.</span></span> <span data-ttu-id="900c1-181">Uppgraderingar av både Service Fabric och dina tjänster följer hello samma modell.</span><span class="sxs-lookup"><span data-stu-id="900c1-181">Upgrades of both Service Fabric and your services follow hello same model.</span></span> <span data-ttu-id="900c1-182">Mer information om rullande uppgraderingar och uppgraderingsdomäner hello Service Fabric-hälsomodell som förhindrar oväntade ändringar påverkar hello klustret och din tjänst finns i dessa dokument:</span><span class="sxs-lookup"><span data-stu-id="900c1-182">For more on rolling upgrades, upgrade domains, and hello Service Fabric health model that helps prevent unintended changes from impacting hello cluster and your service, see these documents:</span></span>

 - [<span data-ttu-id="900c1-183">Uppgradering av programmet</span><span class="sxs-lookup"><span data-stu-id="900c1-183">Application Upgrade</span></span>](service-fabric-application-upgrade.md)
 - [<span data-ttu-id="900c1-184">Uppgradera självstudien</span><span class="sxs-lookup"><span data-stu-id="900c1-184">Application Upgrade Tutorial</span></span>](service-fabric-application-upgrade-tutorial.md)
 - [<span data-ttu-id="900c1-185">Service Fabric-Hälsomodell</span><span class="sxs-lookup"><span data-stu-id="900c1-185">Service Fabric Health Model</span></span>](service-fabric-health-introduction.md)

<span data-ttu-id="900c1-186">Du kan visualisera hello layout på klustret med hjälp av hello över lediga kluster i [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span><span class="sxs-lookup"><span data-stu-id="900c1-186">You can visualize hello layout of your cluster using hello cluster map provided in [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span></span>

<span data-ttu-id="900c1-187"><center>
![Noder som är fördelade på feldomäner i Service Fabric Explorer][sfx-cluster-map]
</center></span><span class="sxs-lookup"><span data-stu-id="900c1-187"><center>
![Nodes spread across fault domains in Service Fabric Explorer][sfx-cluster-map]
</center></span></span>

> [!NOTE]
> <span data-ttu-id="900c1-188">Modeling områden för fel, rullande uppgraderingar, kör flera instanser av Tjänstkod och tillstånd, placering regler tooensure dina tjänster köra fel- och domäner och inbyggda hälsoövervakning är bara **vissa** av hello funktioner som Service Fabric ger i ordning tookeep normala operativa problem och fel från att omvandla till katastrofer.</span><span class="sxs-lookup"><span data-stu-id="900c1-188">Modeling areas of failure, rolling upgrades, running many instances of your service code and state, placement rules tooensure your services run across fault and upgrade domains, and built-in health monitoring are just **some** of hello features that Service Fabric provides in order tookeep normal operational issues and failures from turning into disasters.</span></span> 
>

### <a name="handling-simultaneous-hardware-or-software-failures"></a><span data-ttu-id="900c1-189">Hantering av samtidiga maskinvaru- eller fel</span><span class="sxs-lookup"><span data-stu-id="900c1-189">Handling simultaneous hardware or software failures</span></span>
<span data-ttu-id="900c1-190">Högre diskuterats vi enstaka fel.</span><span class="sxs-lookup"><span data-stu-id="900c1-190">Above we talked about single failures.</span></span> <span data-ttu-id="900c1-191">Som du ser är enkelt toohandle för både tillståndslösa och tillståndskänsliga tjänster genom att hålla fler kopior av hello kod (och tillstånd) körs i fel- och uppgraderingsdomäner.</span><span class="sxs-lookup"><span data-stu-id="900c1-191">As you can see, are easy toohandle for both stateless and stateful services just by keeping more copies of hello code (and state) running across fault and upgrade domains.</span></span> <span data-ttu-id="900c1-192">Flera samtidiga slumpmässiga fel kan också inträffa.</span><span class="sxs-lookup"><span data-stu-id="900c1-192">Multiple simultaneous random failures can also happen.</span></span> <span data-ttu-id="900c1-193">Det här är mer troligt toolead tooan faktiska katastrofåterställning.</span><span class="sxs-lookup"><span data-stu-id="900c1-193">These are more likely toolead tooan actual disaster.</span></span>


### <a name="random-failures-leading-tooservice-failures"></a><span data-ttu-id="900c1-194">Slumpmässiga fel inledande tooservice fel</span><span class="sxs-lookup"><span data-stu-id="900c1-194">Random failures leading tooservice failures</span></span>
<span data-ttu-id="900c1-195">Anta att hello-tjänsten hade en `InstanceCount` 5 och flera noder som kör dessa instanser alla misslyckades vid hello samma tid.</span><span class="sxs-lookup"><span data-stu-id="900c1-195">Let's say that hello service had an `InstanceCount` of 5, and several nodes running those instances all failed at hello same time.</span></span> <span data-ttu-id="900c1-196">Service Fabric svarar genom att automatiskt skapa ersättning instanser på andra noder.</span><span class="sxs-lookup"><span data-stu-id="900c1-196">Service Fabric responds by automatically creating replacement instances on other nodes.</span></span> <span data-ttu-id="900c1-197">Kommer att fortsätta skapa ersättningar tills hello-tjänsten är tillbaka tooits önskat instansantal.</span><span class="sxs-lookup"><span data-stu-id="900c1-197">It will continue creating replacements until hello service is back tooits desired instance count.</span></span> <span data-ttu-id="900c1-198">Ett annat exempel anta att det fanns en tillståndslös tjänst med en `InstanceCount`1, vilket innebär att den körs på alla noder i klustret hello som giltig.</span><span class="sxs-lookup"><span data-stu-id="900c1-198">As another example, let's say there was a stateless service with an `InstanceCount`of -1, meaning it runs on all valid nodes in hello cluster.</span></span> <span data-ttu-id="900c1-199">Anta att några av dessa instanser kunde toofail.</span><span class="sxs-lookup"><span data-stu-id="900c1-199">Let's say that some of those instances were toofail.</span></span> <span data-ttu-id="900c1-200">I det här fallet meddelanden Service Fabric att hello-tjänsten inte är dess status som önskas och försöker toocreate hello instanser på hello noder där de saknas.</span><span class="sxs-lookup"><span data-stu-id="900c1-200">In this case, Service Fabric notices that hello service is not in its desired state, and tries toocreate hello instances on hello nodes where they are missing.</span></span> 

<span data-ttu-id="900c1-201">För tillståndskänsliga tjänster beror hello situationen på om hello-tjänsten har sparat tillstånd eller inte.</span><span class="sxs-lookup"><span data-stu-id="900c1-201">For stateful services hello situation depends on whether hello service has persisted state or not.</span></span> <span data-ttu-id="900c1-202">Det beror också på hur många repliker hello-tjänsten har och hur många misslyckades.</span><span class="sxs-lookup"><span data-stu-id="900c1-202">It also depends on how many replicas hello service had and how many failed.</span></span> <span data-ttu-id="900c1-203">Avgöra om en katastrof uppstod för en tillståndskänslig service och hanteringen av den följer tre steg:</span><span class="sxs-lookup"><span data-stu-id="900c1-203">Determining whether a disaster occurred for a stateful service and managing it follows three stages:</span></span>

1. <span data-ttu-id="900c1-204">Bestämma om det har förekommit förlorar kvorum eller inte</span><span class="sxs-lookup"><span data-stu-id="900c1-204">Determining if there has been quorum loss or not</span></span>
 - <span data-ttu-id="900c1-205">En förlorar kvorum är när en majoritet av hello repliker av en tillståndskänslig service löper hello samtidigt, inklusive hello primär.</span><span class="sxs-lookup"><span data-stu-id="900c1-205">A quorum loss is any time a majority of hello replicas of a stateful service are down at hello same time, including hello Primary.</span></span>
2. <span data-ttu-id="900c1-206">Bestämma om hello kvorumförlust är permanent</span><span class="sxs-lookup"><span data-stu-id="900c1-206">Determining if hello quorum loss is permanent or not</span></span>
 - <span data-ttu-id="900c1-207">De flesta hello tid är fel tillfälligt.</span><span class="sxs-lookup"><span data-stu-id="900c1-207">Most of hello time, failures are transient.</span></span> <span data-ttu-id="900c1-208">Processer har startats om, noder har startats om, virtuella datorer är relaunched, läka nätverkspartitioner.</span><span class="sxs-lookup"><span data-stu-id="900c1-208">Processes are restarted, nodes are restarted, VMs are relaunched, network partitions heal.</span></span> <span data-ttu-id="900c1-209">Ibland om fel är permanent.</span><span class="sxs-lookup"><span data-stu-id="900c1-209">Sometimes though, failures are permanent.</span></span> 
    - <span data-ttu-id="900c1-210">För tjänster utan beständiga tillståndet kan ett fel i minst ett kvorum av repliker resultat _omedelbart_ förlorar kvorum permanent.</span><span class="sxs-lookup"><span data-stu-id="900c1-210">For services without persisted state, a failure of a quorum or more of replicas results _immediately_ in permanent quorum loss.</span></span> <span data-ttu-id="900c1-211">När Service Fabric upptäcker förlorar kvorum i ett icke-beständig tillståndskänslig service, fortsätter omedelbart dataloss toostep 3 genom att deklarera (potentiella).</span><span class="sxs-lookup"><span data-stu-id="900c1-211">When Service Fabric detects quorum loss in a stateful non-persistent service, it immediately proceeds toostep 3 by declaring (potential) dataloss.</span></span> <span data-ttu-id="900c1-212">Om du fortsätter toodataloss meningsfullt eftersom Service Fabric vet att det finns ingen anledning att vänta på hello repliker toocome tillbaka eftersom även om de har återställts de skulle vara tom.</span><span class="sxs-lookup"><span data-stu-id="900c1-212">Proceeding toodataloss makes sense because Service Fabric knows that there's no point in waiting for hello replicas toocome back, because even if they were recovered they would be empty.</span></span>
    - <span data-ttu-id="900c1-213">För tillståndskänsliga beständiga tjänster orsakar ett fel i minst ett kvorum av repliker Service Fabric-toostart väntar hello repliker tillbaka toocome och Återställ kvorum.</span><span class="sxs-lookup"><span data-stu-id="900c1-213">For stateful persistent services, a failure of a quorum or more of replicas causes Service Fabric toostart waiting for hello replicas toocome back and restore quorum.</span></span> <span data-ttu-id="900c1-214">Detta resulterar i ett avbrott för alla _skriver_ toohello påverkas partition (eller ”replikuppsättningen”) för hello-tjänsten.</span><span class="sxs-lookup"><span data-stu-id="900c1-214">This results in a service outage for any _writes_ toohello affected partition (or "replica set") of hello service.</span></span> <span data-ttu-id="900c1-215">Läser kan dock fortfarande vara möjligt med nedsatt konsekvens garanterar.</span><span class="sxs-lookup"><span data-stu-id="900c1-215">However, reads may still be possible with reduced consistency guarantees.</span></span> <span data-ttu-id="900c1-216">hello standardmängden tid som Service Fabric väntar kvorum toobe återställts är oändligt, eftersom du fortsätter är en (eventuella) dataloss händelse och har andra risker.</span><span class="sxs-lookup"><span data-stu-id="900c1-216">hello default amount of time that Service Fabric waits for quorum toobe restored is infinite, since proceeding is a (potential) dataloss event and carries other risks.</span></span> <span data-ttu-id="900c1-217">Åsidosätta hello standard `QuorumLossWaitDuration` värdet går men rekommenderas inte.</span><span class="sxs-lookup"><span data-stu-id="900c1-217">Overriding hello default `QuorumLossWaitDuration` value is possible but is not recommended.</span></span> <span data-ttu-id="900c1-218">I stället för tillfället alla ansträngningar bör göras toorestore hello ned repliker.</span><span class="sxs-lookup"><span data-stu-id="900c1-218">Instead at this time, all efforts should be made toorestore hello down replicas.</span></span> <span data-ttu-id="900c1-219">Detta kräver att hello-noder som är nere tillbaka in och säkerställa att de kan återansluta hello enheter där de lagras hello lokala beständiga tillstånd.</span><span class="sxs-lookup"><span data-stu-id="900c1-219">This requires bringing hello nodes that are down back up, and ensuring that they can remount hello drives where they stored hello local persistent state.</span></span> <span data-ttu-id="900c1-220">Om hello kvorumförlust orsakas av misslyckade försök toorecreate hello processer Service Fabric automatiskt och starta om hello repliker i dem.</span><span class="sxs-lookup"><span data-stu-id="900c1-220">If hello quorum loss is caused by process failure, Service Fabric automatically tries toorecreate hello processes and restart hello replicas inside them.</span></span> <span data-ttu-id="900c1-221">Om detta misslyckas rapporterar Service Fabric hälsa fel.</span><span class="sxs-lookup"><span data-stu-id="900c1-221">If this fails, Service Fabric reports health errors.</span></span> <span data-ttu-id="900c1-222">Om de kan lösas kommer hello repliker vanligtvis tillbaka.</span><span class="sxs-lookup"><span data-stu-id="900c1-222">If these can be resolved then hello replicas usually come back.</span></span> <span data-ttu-id="900c1-223">Ibland kan dock kan hello repliker återtas.</span><span class="sxs-lookup"><span data-stu-id="900c1-223">Sometimes, though, hello replicas can't be brought back.</span></span> <span data-ttu-id="900c1-224">Till exempel hello enheter alla har misslyckats eller hello datorer förstörs fysiskt på något sätt.</span><span class="sxs-lookup"><span data-stu-id="900c1-224">For example, hello drives may all have failed, or hello machines physically destroyed somehow.</span></span> <span data-ttu-id="900c1-225">I dessa fall har vi nu en permanent kvorum dataförlust inträffat.</span><span class="sxs-lookup"><span data-stu-id="900c1-225">In these cases, we now have a permanent quorum loss event.</span></span> <span data-ttu-id="900c1-226">tootell Service Fabric toostop väntar hello ned repliker toocome tillbaka en Klusteradministratör måste avgöra vilka partitioner av vilka tjänster som påverkas och anropa hello `Repair-ServiceFabricPartition -PartitionId` eller ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span><span class="sxs-lookup"><span data-stu-id="900c1-226">tootell Service Fabric toostop waiting for hello down replicas toocome back, a cluster administrator must determine which partitions of which services are affected and call hello `Repair-ServiceFabricPartition -PartitionId` or ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span></span>  <span data-ttu-id="900c1-227">Detta API kan du ange hello-ID för hello partition toomove utanför QuorumLoss och till eventuella dataloss.</span><span class="sxs-lookup"><span data-stu-id="900c1-227">This API allows specifying hello ID of hello partition toomove out of QuorumLoss and into potential dataloss.</span></span>

> [!NOTE]
> <span data-ttu-id="900c1-228">Det är _aldrig_ säker toouse detta API än på ett sätt som är riktad mot specifika partitioner.</span><span class="sxs-lookup"><span data-stu-id="900c1-228">It is _never_ safe toouse this API other than in a targeted way against specific partitions.</span></span> 
>

3. <span data-ttu-id="900c1-229">Avgöra om det har förekommit faktiska data går förlorade och återställning från säkerhetskopior</span><span class="sxs-lookup"><span data-stu-id="900c1-229">Determining if there has been actual data loss, and restoring from backups</span></span>
  - <span data-ttu-id="900c1-230">När Service Fabric anropar hello `OnDataLossAsync` metod är det alltid eftersom _misstänkt_ dataloss.</span><span class="sxs-lookup"><span data-stu-id="900c1-230">When Service Fabric calls hello `OnDataLossAsync` method it is always because of _suspected_ dataloss.</span></span> <span data-ttu-id="900c1-231">Service Fabric garanterar att anropet levereras toohello _bästa_ återstående replik.</span><span class="sxs-lookup"><span data-stu-id="900c1-231">Service Fabric ensures that this call is delivered toohello _best_ remaining replica.</span></span> <span data-ttu-id="900c1-232">Det här är repliken har gjort hello de flesta pågår.</span><span class="sxs-lookup"><span data-stu-id="900c1-232">This is whichever replica has made hello most progress.</span></span> <span data-ttu-id="900c1-233">Hej orsak alltid innebär _misstänkt_ dataloss är att det är möjligt hello återstående repliken faktiskt har alla samma tillstånd som hello primära när den avslutades.</span><span class="sxs-lookup"><span data-stu-id="900c1-233">hello reason we always say _suspected_ dataloss is that it is possible that hello remaining replica actually has all same state as hello Primary did when it went down.</span></span> <span data-ttu-id="900c1-234">Men utan att tillståndet toocompare den till, det går inte bra för Service Fabric eller operatorer tooknow verkligen.</span><span class="sxs-lookup"><span data-stu-id="900c1-234">However, without that state toocompare it to, there's no good way for Service Fabric or operators tooknow for sure.</span></span> <span data-ttu-id="900c1-235">Nu Service Fabric också vet hello repliker inte kommer tillbaka.</span><span class="sxs-lookup"><span data-stu-id="900c1-235">At this point, Service Fabric also knows hello other replicas are not coming back.</span></span> <span data-ttu-id="900c1-236">Som var hello beslut om när vi stoppats och väntar hello kvorum förlust tooresolve sig själv.</span><span class="sxs-lookup"><span data-stu-id="900c1-236">That was hello decision made when we stopped waiting for hello quorum loss tooresolve itself.</span></span> <span data-ttu-id="900c1-237">hello bästa erhåller för hello-tjänsten är vanligtvis toofreeze och vänta tills särskilda administrativa åtgärder.</span><span class="sxs-lookup"><span data-stu-id="900c1-237">hello best course of action for hello service is usually toofreeze and wait for specific administrative intervention.</span></span> <span data-ttu-id="900c1-238">Därför det gör att en typisk implementering av hello `OnDataLossAsync` metoden göra?</span><span class="sxs-lookup"><span data-stu-id="900c1-238">So what does a typical implementation of hello `OnDataLossAsync` method do?</span></span>
  - <span data-ttu-id="900c1-239">Logga först som `OnDataLossAsync` har utlösts och utlösa alla nödvändiga administrativa varningar.</span><span class="sxs-lookup"><span data-stu-id="900c1-239">First, log that `OnDataLossAsync` has been triggered, and fire off any necessary administrative alerts.</span></span>
   - <span data-ttu-id="900c1-240">Vanligtvis nu toopause och vänta tills ytterligare beslut och manuella åtgärder toobe vidtas.</span><span class="sxs-lookup"><span data-stu-id="900c1-240">Usually at this point, toopause and wait for further decisions and manual actions toobe taken.</span></span> <span data-ttu-id="900c1-241">Det beror på att även om säkerhetskopior är tillgängliga kan de behöver toobe förberedd.</span><span class="sxs-lookup"><span data-stu-id="900c1-241">This is because even if backups are available they may need toobe prepared.</span></span> <span data-ttu-id="900c1-242">Om två olika tjänster samordna information, till exempel måste säkerhetskopieringarna toobe ändras i ordning tooensure som när hello återställning händer att hello information dessa två tjänster är intresserad är konsekvent.</span><span class="sxs-lookup"><span data-stu-id="900c1-242">For example, if two different services coordinate information, those backups may need toobe modified in order tooensure that once hello restore happens that hello information those two services care about is consistent.</span></span> 
  - <span data-ttu-id="900c1-243">Ofta finns även vissa andra telemetri eller avgaser från hello-tjänsten.</span><span class="sxs-lookup"><span data-stu-id="900c1-243">Often there is also some other telemetry or exhaust from hello service.</span></span> <span data-ttu-id="900c1-244">Dessa metadata kan finnas i loggarna eller i andra tjänster.</span><span class="sxs-lookup"><span data-stu-id="900c1-244">This metadata may be contained in other services or in logs.</span></span> <span data-ttu-id="900c1-245">Den här informationen kan vara används behövs toodetermine om det inte fanns några anrop emot och bearbetas på hello primära som inte fanns i hello säkerhetskopiering eller replikerade toothis viss replik.</span><span class="sxs-lookup"><span data-stu-id="900c1-245">This information can be used needed toodetermine if there were any calls received and processed at hello primary that were not present in hello backup or replicated toothis particular replica.</span></span> <span data-ttu-id="900c1-246">Dessa kräver toobe upprepat eller har lagts till toohello säkerhetskopiering innan det är möjligt att återställningen.</span><span class="sxs-lookup"><span data-stu-id="900c1-246">These may need toobe replayed or added toohello backup before restoration is feasible.</span></span>  
   - <span data-ttu-id="900c1-247">Jämförelser av hello återstående replikens tillstånd toothat som ingår i alla säkerhetskopior som är tillgängliga.</span><span class="sxs-lookup"><span data-stu-id="900c1-247">Comparisons of hello remaining replica's state toothat contained in any backups that are available.</span></span> <span data-ttu-id="900c1-248">Om du använder hello Service Fabric tillförlitliga samlingar finns det verktyg och bearbetar tillgängliga för att göra det, beskrivs i [i den här artikeln](service-fabric-reliable-services-backup-restore.md).</span><span class="sxs-lookup"><span data-stu-id="900c1-248">If using hello Service Fabric reliable collections then there are tools and processes available for doing so, described in [this article](service-fabric-reliable-services-backup-restore.md).</span></span> <span data-ttu-id="900c1-249">hello målet är toosee om hello tillstånd i hello replik räcker, eller också vilka hello-säkerhetskopiering kanske saknas.</span><span class="sxs-lookup"><span data-stu-id="900c1-249">hello goal is toosee if hello state within hello replica is sufficient, or also what hello backup may be missing.</span></span>
  - <span data-ttu-id="900c1-250">En gång hello jämförelsen görs och om nödvändigt hello återställningen har slutförts, hello service-kod ska returnera true om alla tillstånd har ändrats.</span><span class="sxs-lookup"><span data-stu-id="900c1-250">Once hello comparison is done, and if necessary hello restore completed, hello service code should return true if any state changes were made.</span></span> <span data-ttu-id="900c1-251">Om hello replik har fastställt att det var hello bäst tillgängliga kopia av hello tillstånd och gjort några ändringar, returnera false.</span><span class="sxs-lookup"><span data-stu-id="900c1-251">If hello replica determined that it was hello best available copy of hello state and made no changes, then return false.</span></span> <span data-ttu-id="900c1-252">SANT anger att de _andra_ återstående repliker kan nu inte stämmer överens med den här.</span><span class="sxs-lookup"><span data-stu-id="900c1-252">True indicates that any _other_ remaining replicas may now be inconsistent with this one.</span></span> <span data-ttu-id="900c1-253">De tas bort och återskapas från den här repliken.</span><span class="sxs-lookup"><span data-stu-id="900c1-253">They will be dropped and rebuilt from this replica.</span></span> <span data-ttu-id="900c1-254">Falskt anger att gjordes inga tillståndsändringar, så hello repliker kan behålla de har.</span><span class="sxs-lookup"><span data-stu-id="900c1-254">False indicates that no state changes were made, so hello other replicas can keep what they have.</span></span> 

<span data-ttu-id="900c1-255">Det är ytterst viktigt att tjänsten författare öva potentiella dataloss och scenarier innan tjänster aldrig har distribuerats i produktionsmiljön.</span><span class="sxs-lookup"><span data-stu-id="900c1-255">It is critically important that service authors practice potential dataloss and failure scenarios before services are ever deployed in production.</span></span> <span data-ttu-id="900c1-256">tooprotect mot hello möjligheten att dataloss är det viktigt tooperiodically [säkerhetskopiera hello tillstånd](service-fabric-reliable-services-backup-restore.md) för någon av dina tillståndskänsliga tjänster tooa geo-redundant lagring.</span><span class="sxs-lookup"><span data-stu-id="900c1-256">tooprotect against hello possibility of dataloss, it is important tooperiodically [back up hello state](service-fabric-reliable-services-backup-restore.md) of any of your stateful services tooa geo-redundant store.</span></span> <span data-ttu-id="900c1-257">Du måste också kontrollera att du har hello möjlighet toorestore den.</span><span class="sxs-lookup"><span data-stu-id="900c1-257">You must also ensure that you have hello ability toorestore it.</span></span> <span data-ttu-id="900c1-258">Eftersom säkerhetskopior av många olika tjänster utförs vid olika tidpunkter, måste tooensure att tjänsterna efter en återställning har en konsekvent överblick över varandra.</span><span class="sxs-lookup"><span data-stu-id="900c1-258">Since backups of many different services are taken at different times, you need tooensure that after a restore your services have a consistent view of each other.</span></span> <span data-ttu-id="900c1-259">Anta till exempel att en situation där en tjänst genererar ett tal och lagrar det och skickar den tooanother tjänst som lagrar även det.</span><span class="sxs-lookup"><span data-stu-id="900c1-259">For example, consider a situation where one service generates a number and stores it, then sends it tooanother service that also stores it.</span></span> <span data-ttu-id="900c1-260">Efter en återställning kan du identifiera att andra hello-tjänsten har hello nummer men hello först inte, eftersom dess säkerhetskopieringen inte innehåller denna åtgärd.</span><span class="sxs-lookup"><span data-stu-id="900c1-260">After a restore, you might discover that hello second service has hello number but hello first does not, because it's backup didn't include that operation.</span></span>

<span data-ttu-id="900c1-261">Om du upptäcker att hello återstående repliker är otillräcklig toocontinue från i ett scenario med dataloss och du kan inte återskapa Tjänststatus från telemetri eller avgaser, avgör hello frekvens för dina säkerhetskopieringar det bästa möjliga återställningspunktmålet (RPO) .</span><span class="sxs-lookup"><span data-stu-id="900c1-261">If you find out that hello remaining replicas are insufficient toocontinue from in a dataloss scenario, and you can't reconstruct service state from telemetry or exhaust, hello frequency of your backups determines your best possible recovery point objective (RPO).</span></span> <span data-ttu-id="900c1-262">Service Fabric innehåller många verktyg för att testa olika scenarier, inklusive permanent kvorum och dataloss som kräver återställning från en säkerhetskopia.</span><span class="sxs-lookup"><span data-stu-id="900c1-262">Service Fabric provides many tools for testing various failure scenarios, including permanent quorum and dataloss requiring restoration from a backup.</span></span> <span data-ttu-id="900c1-263">Dessa scenarier ingår som en del av Service Fabric datatillgång verktyg, hanteras av hello fel Analysis Services.</span><span class="sxs-lookup"><span data-stu-id="900c1-263">These scenarios are included as a part of Service Fabric's testability tools, managed by hello Fault Analysis Service.</span></span> <span data-ttu-id="900c1-264">Mer information om dessa verktyg och mönster finns [här](service-fabric-testability-overview.md).</span><span class="sxs-lookup"><span data-stu-id="900c1-264">More info on those tools and patterns is available [here](service-fabric-testability-overview.md).</span></span> 

> [!NOTE]
> <span data-ttu-id="900c1-265">Systemtjänster kan också påverkas kvorum försvinner, hello påverkas som specifika toohello tjänsten i fråga.</span><span class="sxs-lookup"><span data-stu-id="900c1-265">System services can also suffer quorum loss, with hello impact being specific toohello service in question.</span></span> <span data-ttu-id="900c1-266">Till exempel påverkar kvorumförlust i hello namngivningstjänst namnmatchning, medan förlorar kvorum i hello failover manager service blockerar skapandet av ny tjänst och växling vid fel.</span><span class="sxs-lookup"><span data-stu-id="900c1-266">For instance, quorum loss in hello naming service impacts name resolution, whereas quorum loss in hello failover manager service blocks new service creation and failovers.</span></span> <span data-ttu-id="900c1-267">Medan hello Service Fabric-systemtjänster följer samma mönster som dina tjänster för tillståndshantering av hello, rekommenderas inte att du ska försöka toomove dem utanför förlorar kvorum och till eventuella dataloss.</span><span class="sxs-lookup"><span data-stu-id="900c1-267">While hello Service Fabric system services follow hello same pattern as your services for state management, it is not recommended that you should attempt toomove them out of Quorum Loss and into potential dataloss.</span></span> <span data-ttu-id="900c1-268">hello rekommendation är i stället för[söka stöd](service-fabric-support.md) toodetermine en lösning som är riktade tooyour specifika situation.</span><span class="sxs-lookup"><span data-stu-id="900c1-268">hello recommendation is instead too[seek support](service-fabric-support.md) toodetermine a solution that is targeted tooyour specific situation.</span></span>  <span data-ttu-id="900c1-269">Vanligtvis är det bättre toosimply vänta tills hello ned repliker returtyp.</span><span class="sxs-lookup"><span data-stu-id="900c1-269">Usually it is preferable toosimply wait until hello down replicas return.</span></span>
>

## <a name="availability-of-hello-service-fabric-cluster"></a><span data-ttu-id="900c1-270">Tillgängligheten för hello Service Fabric-kluster</span><span class="sxs-lookup"><span data-stu-id="900c1-270">Availability of hello Service Fabric cluster</span></span>
<span data-ttu-id="900c1-271">Generellt sett är hello Service Fabric själva klustret en miljö med några enskilda felpunkter.</span><span class="sxs-lookup"><span data-stu-id="900c1-271">Generally speaking, hello Service Fabric cluster itself is a highly distributed environment with no single points of failure.</span></span> <span data-ttu-id="900c1-272">Ett fel på någon en nod inte ska orsaka tillgänglighet eller tillförlitlighetsproblem för hello-kluster i första hand eftersom hello Service Fabric-systemtjänster följa samma riktlinjer som tidigare hello: de alltid körs med tre eller flera repliker som standard och de systemtjänster som är tillståndslös köras på alla noder.</span><span class="sxs-lookup"><span data-stu-id="900c1-272">A failure of any one node will not cause availability or reliability issues for hello cluster, primarily because hello Service Fabric system services follow hello same guidelines provided earlier: they always run with three or more replicas by default, and those system services that are stateless run on all nodes.</span></span> <span data-ttu-id="900c1-273">hello underliggande Service Fabric-nätverk och lager för identifiering av fel är helt distribuerade.</span><span class="sxs-lookup"><span data-stu-id="900c1-273">hello underlying Service Fabric networking and failure detection layers are fully distributed.</span></span> <span data-ttu-id="900c1-274">De flesta systemtjänster kan byggas från metadata i hello kluster eller vet hur tooresynchronize deras tillstånd från andra platser.</span><span class="sxs-lookup"><span data-stu-id="900c1-274">Most system services can be rebuilt from metadata in hello cluster, or know how tooresynchronize their state from other places.</span></span> <span data-ttu-id="900c1-275">hello tillgängligheten för hello klustret kan bli komprometterade om systemtjänster komma in kvorum förlust situationer som de som beskrivs ovan.</span><span class="sxs-lookup"><span data-stu-id="900c1-275">hello availability of hello cluster can become compromised if system services get into quorum loss situations like those described above.</span></span> <span data-ttu-id="900c1-276">I dessa fall kanske inte kan tooperform vissa åtgärder på hello klustret som påbörjar en uppgradering eller distribuera nya tjänster, men själva hello klustret är fortfarande in.</span><span class="sxs-lookup"><span data-stu-id="900c1-276">In these cases you may not be able tooperform certain operations on hello cluster like starting an upgrade or deploying new services, but hello cluster itself is still up.</span></span> <span data-ttu-id="900c1-277">Tjänster på redan körs fortsätter att köras i dessa villkor om de inte behöver skrivningar toohello system services toocontinue fungerar.</span><span class="sxs-lookup"><span data-stu-id="900c1-277">Services on already running will remain running in these conditions unless they require writes toohello system services toocontinue functioning.</span></span> <span data-ttu-id="900c1-278">Till exempel om hello Failover Manager är förlorar kvorum alla tjänster fortsätter toorun, men alla tjänster som inte kommer inte att kunna tooautomatically omstart, eftersom det kräver hello involvering av hello Failover Manager.</span><span class="sxs-lookup"><span data-stu-id="900c1-278">For example, if hello Failover Manager is in quorum loss all services will continue toorun, but any services that fail will not be able tooautomatically restart, since this requires hello involvement of hello Failover Manager.</span></span> 

### <a name="failures-of-a-datacenter-or-azure-region"></a><span data-ttu-id="900c1-279">Fel i en datacenter- eller Azure-region</span><span class="sxs-lookup"><span data-stu-id="900c1-279">Failures of a datacenter or Azure region</span></span>
<span data-ttu-id="900c1-280">I sällsynta fall kan ett fysiska Datacenter blir tillfälligt otillgänglig på grund av tooloss av power eller nätverksanslutning.</span><span class="sxs-lookup"><span data-stu-id="900c1-280">In rare cases, a physical data center can become temporarily unavailable due tooloss of power or network connectivity.</span></span> <span data-ttu-id="900c1-281">I dessa fall måste blir din Service Fabric-kluster och tjänster i datacenter eller Azure-region otillgänglig.</span><span class="sxs-lookup"><span data-stu-id="900c1-281">In these cases, your Service Fabric clusters and services in that datacenter or Azure region will be unavailable.</span></span> <span data-ttu-id="900c1-282">Dock _dina data bevaras_.</span><span class="sxs-lookup"><span data-stu-id="900c1-282">However, _your data is preserved_.</span></span> <span data-ttu-id="900c1-283">För kluster som körs i Azure, kan du visa uppdateringar på avbrott på hello [status för Azure][azure-status-dashboard].</span><span class="sxs-lookup"><span data-stu-id="900c1-283">For clusters running in Azure, you can view updates on outages on hello [Azure status page][azure-status-dashboard].</span></span> <span data-ttu-id="900c1-284">Hög osannolika fysiska Datacenter är helt eller delvis förstörs alla Service Fabric-kluster finns det i hello eller hello tjänster i dem kan gå förlorade.</span><span class="sxs-lookup"><span data-stu-id="900c1-284">In hello highly unlikely event that a physical data center is partially or fully destroyed, any Service Fabric clusters hosted there or hello services inside them could be lost.</span></span> <span data-ttu-id="900c1-285">Detta inkluderar några tillstånd som inte säkerhetskopieras utanför det datacenter eller regionen.</span><span class="sxs-lookup"><span data-stu-id="900c1-285">This includes any state not backed up outside of that datacenter or region.</span></span>

<span data-ttu-id="900c1-286">Det finns två olika strategier för kvarvarande hello permanent eller varaktigt fel i ett enda datacenter eller en region.</span><span class="sxs-lookup"><span data-stu-id="900c1-286">There's two different strategies for surviving hello permanent or sustained failure of a single datacenter or region.</span></span> 

1. <span data-ttu-id="900c1-287">Kör separata Service Fabric-kluster i flera områden, och använda någon mekanism för växling vid fel och redundansväxla mellan dessa miljöer.</span><span class="sxs-lookup"><span data-stu-id="900c1-287">Run separate Service Fabric clusters in multiple such regions, and utilize some mechanism for failover and fail-back between these environments.</span></span> <span data-ttu-id="900c1-288">Den här typen av flera klustermodellen för aktiv-aktiv eller aktivt-passivt kräver ytterligare kod för hantering och åtgärder.</span><span class="sxs-lookup"><span data-stu-id="900c1-288">This sort of multi-cluster active-active or active-passive model requires additional management and operations code.</span></span> <span data-ttu-id="900c1-289">Detta kräver samordning av säkerhetskopior från hello tjänster i ett datacenter eller region så att de är tillgängliga i andra Datacenter när en misslyckas.</span><span class="sxs-lookup"><span data-stu-id="900c1-289">This also requires coordination of backups from hello services in one datacenter or region so that they are available in other datacenters or regions when one fails.</span></span> 
2. <span data-ttu-id="900c1-290">Kör ett enda Service Fabric-kluster som omfattar flera Datacenter eller regioner.</span><span class="sxs-lookup"><span data-stu-id="900c1-290">Run a single Service Fabric cluster that spans multiple datacenters or regions.</span></span> <span data-ttu-id="900c1-291">hello minimikraven på konfigurationen för det här är tre Datacenter eller regioner.</span><span class="sxs-lookup"><span data-stu-id="900c1-291">hello minimum supported configuration for this is three datacenters or regions.</span></span> <span data-ttu-id="900c1-292">hello rekommenderade antalet regioner eller Datacenter är fem.</span><span class="sxs-lookup"><span data-stu-id="900c1-292">hello recommended number of regions or datacenters is five.</span></span> <span data-ttu-id="900c1-293">Detta kräver en mer komplex klustertopologi.</span><span class="sxs-lookup"><span data-stu-id="900c1-293">This requires a more complex cluster topology.</span></span> <span data-ttu-id="900c1-294">Hello är fördelen med den här modellen dock att fel i ett datacenter eller region konverteras till ett vanligt fel från en katastrof.</span><span class="sxs-lookup"><span data-stu-id="900c1-294">However, hello benefit of this model is that failure of one datacenter or region is converted from a disaster into a normal failure.</span></span> <span data-ttu-id="900c1-295">Dessa fel kan hanteras av hello mekanismer som fungerar för kluster inom en enskild region.</span><span class="sxs-lookup"><span data-stu-id="900c1-295">These failures can be handled by hello mechanisms that work for clusters within a single region.</span></span> <span data-ttu-id="900c1-296">Se till arbetsbelastningar distribueras så att de tolerera normal fel feldomäner, uppgraderingsdomäner och regler för Service Fabric-placering.</span><span class="sxs-lookup"><span data-stu-id="900c1-296">Fault domains, upgrade domains, and Service Fabric's placement rules ensure workloads are distributed so that they tolerate normal failures.</span></span> <span data-ttu-id="900c1-297">Mer information om principer som kan hjälpa dig att använda tjänster i den här typen av klustret läsa på [placeringsprinciper](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span><span class="sxs-lookup"><span data-stu-id="900c1-297">For more information on policies that can help operate services in this type of cluster, read up on [placement policies](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span></span>

### <a name="random-failures-leading-toocluster-failures"></a><span data-ttu-id="900c1-298">Slumpmässiga fel inledande toocluster fel</span><span class="sxs-lookup"><span data-stu-id="900c1-298">Random failures leading toocluster failures</span></span>
<span data-ttu-id="900c1-299">Service Fabric har hello begreppet Seed-noder.</span><span class="sxs-lookup"><span data-stu-id="900c1-299">Service Fabric has hello concept of Seed Nodes.</span></span> <span data-ttu-id="900c1-300">Dessa är noder som underhåller hello tillgängligheten för underliggande hello-klustret.</span><span class="sxs-lookup"><span data-stu-id="900c1-300">These are nodes that maintain hello availability of hello underlying cluster.</span></span> <span data-ttu-id="900c1-301">Dessa noder hjälpa tooensure hello klustret fortfarande in genom att upprätta lån med andra noder och fungerar som tiebreakers under vissa typer av nätverksfel.</span><span class="sxs-lookup"><span data-stu-id="900c1-301">These nodes help tooensure hello cluster remains up by establishing leases with other nodes and serving as tiebreakers during certain kinds of network failures.</span></span> <span data-ttu-id="900c1-302">Om de inte återförts, slumpmässiga fel ta bort en majoritet av hello seed noder i klustret hello stängs hello klustret automatiskt.</span><span class="sxs-lookup"><span data-stu-id="900c1-302">If random failures remove a majority of hello seed nodes in hello cluster and they are not brought back, hello cluster automatically shuts down.</span></span> <span data-ttu-id="900c1-303">I Azure, hanteras automatiskt Seed noder: de distribueras över hello tillgängliga fel- och uppgraderingsdomäner och om en enda seed-nod har tagits bort från hello klustret skapas en annan i dess ställe.</span><span class="sxs-lookup"><span data-stu-id="900c1-303">In Azure, Seed Nodes are automatically managed: they are distributed over hello available fault and upgrade domains, and if a single seed node is removed from hello cluster another one will be created in its place.</span></span> 

<span data-ttu-id="900c1-304">I både fristående Service Fabric-kluster och Azure är hello ”primära nodtypen” hello en som kör hello frö.</span><span class="sxs-lookup"><span data-stu-id="900c1-304">In both standalone Service Fabric clusters and Azure, hello "Primary Node Type" is hello one that runs hello seeds.</span></span> <span data-ttu-id="900c1-305">När du definierar en primära nodtypen Service Fabric automatiskt att dra nytta av hello antalet noder som tillhandahålls av skapar too9 seed noder och 9 repliker för varje hello systemtjänster.</span><span class="sxs-lookup"><span data-stu-id="900c1-305">When defining a primary node type, Service Fabric will automatically take advantage of hello number of nodes provided by creating up too9 seed nodes and 9 replicas of each of hello system services.</span></span> <span data-ttu-id="900c1-306">Om en uppsättning slumpmässiga fel tar ut en majoritet av dessa system service repliker samtidigt ange hello systemtjänster kvorum försvinner, som det beskrivs ovan.</span><span class="sxs-lookup"><span data-stu-id="900c1-306">If a set of random failures takes out a majority of those system service replicas simultaneously, hello system services will enter quorum loss, as we described above.</span></span> <span data-ttu-id="900c1-307">Om en majoritet av hello seed noder går förlorade, stängs hello klustret strax efter.</span><span class="sxs-lookup"><span data-stu-id="900c1-307">If a majority of hello seed nodes are lost, hello cluster will shut down soon after.</span></span>

## <a name="next-steps"></a><span data-ttu-id="900c1-308">Nästa steg</span><span class="sxs-lookup"><span data-stu-id="900c1-308">Next steps</span></span>
- <span data-ttu-id="900c1-309">Lär dig hur toosimulate olika fel med hello [datatillgång framework](service-fabric-testability-overview.md)</span><span class="sxs-lookup"><span data-stu-id="900c1-309">Learn how toosimulate various failures using hello [testability framework](service-fabric-testability-overview.md)</span></span>
- <span data-ttu-id="900c1-310">Läs andra resurser för katastrofåterställning och hög tillgänglighet.</span><span class="sxs-lookup"><span data-stu-id="900c1-310">Read other disaster-recovery and high-availability resources.</span></span> <span data-ttu-id="900c1-311">Microsoft har publicerat en stor mängd information i dessa avsnitt.</span><span class="sxs-lookup"><span data-stu-id="900c1-311">Microsoft has published a large amount of guidance on these topics.</span></span> <span data-ttu-id="900c1-312">Även om vissa av de här dokumenten finns toospecific tekniker för användning i andra produkter innehåller många allmänna metodtips som du kan använda i hello Service Fabric-kontexten också:</span><span class="sxs-lookup"><span data-stu-id="900c1-312">While some of these documents refer toospecific techniques for use in other products, they contain many general best practices you can apply in hello Service Fabric context as well:</span></span>
  - [<span data-ttu-id="900c1-313">Tillgänglighetschecklista</span><span class="sxs-lookup"><span data-stu-id="900c1-313">Availability checklist</span></span>](../best-practices-availability-checklist.md)
  - [<span data-ttu-id="900c1-314">Utför en katastrofåterställning återställningsgranskning</span><span class="sxs-lookup"><span data-stu-id="900c1-314">Performing a disaster recovery drill</span></span>](../sql-database/sql-database-disaster-recovery-drills.md)
  - <span data-ttu-id="900c1-315">[Katastrofåterställning och hög tillgänglighet för Azure-program][dr-ha-guide]</span><span class="sxs-lookup"><span data-stu-id="900c1-315">[Disaster recovery and high availability for Azure applications][dr-ha-guide]</span></span>
- <span data-ttu-id="900c1-316">Lär dig mer om [Service Fabric-supportalternativen](service-fabric-support.md)</span><span class="sxs-lookup"><span data-stu-id="900c1-316">Learn about [Service Fabric support options](service-fabric-support.md)</span></span>

<!-- External links -->

[repair-partition-ps]: https://msdn.microsoft.com/library/mt163522.aspx
[azure-status-dashboard]:https://azure.microsoft.com/status/
[azure-regions]: https://azure.microsoft.com/regions/
[dr-ha-guide]: https://msdn.microsoft.com/library/azure/dn251004.aspx


<!-- Images -->

[sfx-cluster-map]: ./media/service-fabric-disaster-recovery/sfx-clustermap.png

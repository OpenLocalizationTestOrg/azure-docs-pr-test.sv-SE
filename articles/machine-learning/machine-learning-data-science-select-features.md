---
title: aaaFeature val i hello Team datavetenskap Process | Microsoft Docs
description: "Beskriver hello syftet val av funktioner och innehåller exempel på deras roll i hello förbättring av data för machine learning."
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 878541f5-1df8-4368-889a-ced6852aba47
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/24/2017
ms.author: zhangya;bradsev
ms.openlocfilehash: 54af93c83e4cc6a3670b3ad62490e0f74082b4ee
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: sv-SE
ms.lasthandoff: 10/06/2017
---
# <a name="feature-selection-in-hello-team-data-science-process-tdsp"></a><span data-ttu-id="80d4a-103">Val av funktioner i hello Team Data vetenskap processen (TDSP)</span><span class="sxs-lookup"><span data-stu-id="80d4a-103">Feature selection in hello Team Data Science Process (TDSP)</span></span>
<span data-ttu-id="80d4a-104">Den här artikeln beskriver hello tillämpningen av val av funktioner och innehåller exempel på sin roll i hello förbättring av data för machine learning.</span><span class="sxs-lookup"><span data-stu-id="80d4a-104">This article explains hello purposes of feature selection and provides examples of its role in hello data enhancement process of machine learning.</span></span> <span data-ttu-id="80d4a-105">Dessa exempel hämtas från Azure Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="80d4a-105">These examples are drawn from Azure Machine Learning Studio.</span></span> 

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="80d4a-106">Hej tekniker och funktioner är en del av hello Team Data vetenskap processen (TDSP) beskrivs i [vad är hello Team av vetenskapliga data?](data-science-process-overview.md).</span><span class="sxs-lookup"><span data-stu-id="80d4a-106">hello engineering and selection of features is one part of hello Team Data Science Process (TDSP) outlined in [What is hello Team Data Science Process?](data-science-process-overview.md).</span></span> <span data-ttu-id="80d4a-107">Funktionen tekniker och markeringen är delar av hello **utveckla funktioner** steg i hello TDSP.</span><span class="sxs-lookup"><span data-stu-id="80d4a-107">Feature engineering and selection are parts of hello **Develop features** step of hello TDSP.</span></span>

* <span data-ttu-id="80d4a-108">**egenskapsval**: den här processen försöker toocreate ytterligare relevanta funktioner från hello befintliga raw-funktioner i hello data och tooincrease förutsägande power toohello Inlärningsalgoritmen.</span><span class="sxs-lookup"><span data-stu-id="80d4a-108">**feature engineering**: This process attempts toocreate additional relevant features from hello existing raw features in hello data, and tooincrease predictive power toohello learning algorithm.</span></span>
* <span data-ttu-id="80d4a-109">**funktionen val**: den här processen väljer hello viktiga delmängd av den ursprungliga datafunktioner i ett försök tooreduce hello dimensionalitet hello utbildning problemet.</span><span class="sxs-lookup"><span data-stu-id="80d4a-109">**feature selection**: This process selects hello key subset of original data features in an attempt tooreduce hello dimensionality of hello training problem.</span></span>

<span data-ttu-id="80d4a-110">Normalt **egenskapsval** är tillämpade första toogenerate ytterligare funktioner och hello **funktion markeringen** steget är utförs tooeliminate irrelevanta, redundant eller hög korrelerade funktioner.</span><span class="sxs-lookup"><span data-stu-id="80d4a-110">Normally **feature engineering** is applied first toogenerate additional features, and then hello **feature selection** step is performed tooeliminate irrelevant, redundant, or highly correlated features.</span></span>

## <a name="filtering-features-from-your-data---feature-selection"></a><span data-ttu-id="80d4a-111">Filtrera funktioner från dina Data - val av funktioner</span><span class="sxs-lookup"><span data-stu-id="80d4a-111">Filtering Features from Your Data - Feature Selection</span></span>
<span data-ttu-id="80d4a-112">Val av funktioner är en process som används ofta för hello konstruktion av utbildning datamängder för förutsägelsemodellering aktiviteter, till exempel klassificerings- eller regressionsmodell uppgifter.</span><span class="sxs-lookup"><span data-stu-id="80d4a-112">Feature selection is a process that is commonly applied for hello construction of training datasets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="80d4a-113">hello målet är tooselect en delmängd av hello funktioner från hello ursprungliga datauppsättningen som minskar dess storlek genom att använda en minimal uppsättning funktioner toorepresent hello maximal mängd varians i hello data.</span><span class="sxs-lookup"><span data-stu-id="80d4a-113">hello goal is tooselect a subset of hello features from hello original dataset that reduce its dimensions by using a minimal set of features toorepresent hello maximum amount of variance in hello data.</span></span> <span data-ttu-id="80d4a-114">Den här delmängd av funktionerna är sedan hello endast funktioner toobe ingår tootrain hello modellen.</span><span class="sxs-lookup"><span data-stu-id="80d4a-114">This subset of features are, then, hello only features toobe included tootrain hello model.</span></span> <span data-ttu-id="80d4a-115">Val av funktioner har två huvudsakliga syften.</span><span class="sxs-lookup"><span data-stu-id="80d4a-115">Feature selection serves two main purposes.</span></span>

* <span data-ttu-id="80d4a-116">Först Funktionsurval ökar ofta klassificering noggrannhet genom att ta bort irrelevanta, redundanta eller hög korrelerade funktioner.</span><span class="sxs-lookup"><span data-stu-id="80d4a-116">First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="80d4a-117">Dessutom minskas hello antal funktioner som gör modellen utbildning processen mer effektiv.</span><span class="sxs-lookup"><span data-stu-id="80d4a-117">Second, it decreases hello number of features which makes model training process more efficient.</span></span> <span data-ttu-id="80d4a-118">Detta är särskilt viktigt för inlärning är dyr tootrain som support vector datorer.</span><span class="sxs-lookup"><span data-stu-id="80d4a-118">This is particularly important for learners that are expensive tootrain such as support vector machines.</span></span>

<span data-ttu-id="80d4a-119">Även om Funktionsurval avser tooreduce hello antal funktioner i hello dataset används tootrain hello modellen, är det inte vanligtvis enligt tooby hello termen ”dimensionalitet minskning”.</span><span class="sxs-lookup"><span data-stu-id="80d4a-119">Although feature selection does seek tooreduce hello number of features in hello dataset used tootrain hello model, it is not usually referred tooby hello term "dimensionality reduction".</span></span> <span data-ttu-id="80d4a-120">Funktionen val metoder extrahera en delmängd av ursprungliga funktioner i hello data utan att ändra dem.</span><span class="sxs-lookup"><span data-stu-id="80d4a-120">Feature selection methods extract a subset of original features in hello data without changing them.</span></span>  <span data-ttu-id="80d4a-121">Metoder för dimensionalitet minskning av bakåtkompilerade funktioner som kan omvandla hello ursprungliga funktioner och därmed ändra dem.</span><span class="sxs-lookup"><span data-stu-id="80d4a-121">Dimensionality reduction methods employ engineered features that can transform hello original features and thus modify them.</span></span> <span data-ttu-id="80d4a-122">Exempel på dimensionalitet minskning metoder är Principal komponenten analys, kanoniska korrelation analys och enkel värdet uppdelning.</span><span class="sxs-lookup"><span data-stu-id="80d4a-122">Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</span></span>

<span data-ttu-id="80d4a-123">Bland annat kallas en allmänt tillämpade kategori av funktionen markeringsmetoder i en övervakad kontext ”filter baserat Funktionsurval”.</span><span class="sxs-lookup"><span data-stu-id="80d4a-123">Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</span></span> <span data-ttu-id="80d4a-124">Genom att utvärdera hello korrelation mellan varje funktion och hello målattribut gäller metoderna statistiskt mått-tooassign en poäng tooeach funktion.</span><span class="sxs-lookup"><span data-stu-id="80d4a-124">By evaluating hello correlation between each feature and hello target attribute, these methods apply a statistical measure tooassign a score tooeach feature.</span></span> <span data-ttu-id="80d4a-125">hello funktioner rangordnas sedan hello poäng, vilket kan vara används toohelp angiven hello tröskel för att hålla eller ta bort en specifik funktion.</span><span class="sxs-lookup"><span data-stu-id="80d4a-125">hello features are then ranked by hello score, which may be used toohelp set hello threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="80d4a-126">Exempel på hello statistiskt mått som används i dessa metoder är Person korrelation ömsesidig information och hello Chi kvadraten test.</span><span class="sxs-lookup"><span data-stu-id="80d4a-126">Examples of hello statistical measures used in these methods include Person correlation, mutual information, and hello Chi squared test.</span></span>

<span data-ttu-id="80d4a-127">Det finns moduler som angetts för val av funktioner i Azure Machine Learning Studio.</span><span class="sxs-lookup"><span data-stu-id="80d4a-127">In Azure Machine Learning Studio, there are modules provided for feature selection.</span></span> <span data-ttu-id="80d4a-128">I följande bild hello visas dessa moduler innehåller [Filter-baserade Funktionsurval] [ filter-based-feature-selection] och [Fisher linjär Discriminant analys] [ fisher-linear-discriminant-analysis].</span><span class="sxs-lookup"><span data-stu-id="80d4a-128">As shown in hello following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![Funktionen val exempel](./media/machine-learning-data-science-select-features/feature-Selection.png)

<span data-ttu-id="80d4a-130">Tänk till exempel hello av hello [Filter-baserade Funktionsurval] [ filter-based-feature-selection] modul.</span><span class="sxs-lookup"><span data-stu-id="80d4a-130">Consider, for example, hello use of hello [Filter-Based Feature Selection][filter-based-feature-selection] module.</span></span> <span data-ttu-id="80d4a-131">I hello syftet att underlätta för dig fortsätta vi toouse hello datautvinning textexempel ovan.</span><span class="sxs-lookup"><span data-stu-id="80d4a-131">For hello purpose of convenience, we continue toouse hello text mining example outlined above.</span></span> <span data-ttu-id="80d4a-132">Anta att vi vill toobuild en regressionsmodell efter en uppsättning 256 funktioner har skapats via hello [hash-funktionen] [ feature-hashing] modulen och hello svar variabeln är hello ”Kol1” och representerar en bok Granska mellan 1 too5.</span><span class="sxs-lookup"><span data-stu-id="80d4a-132">Assume that we want toobuild a regression model after a set of 256 features are created through hello [Feature Hashing][feature-hashing] module, and that hello response variable is hello "Col1" and represents a book review ratings ranging from 1 too5.</span></span> <span data-ttu-id="80d4a-133">Genom att ange ”funktionen bedömningen metoden” toobe ”kvadratvärdet korrelation” hello ”målkolumnen” toobe ”Kol1” och hello ”antal önskade funktioner” too50.</span><span class="sxs-lookup"><span data-stu-id="80d4a-133">By setting "Feature scoring method" toobe "Pearson Correlation", hello "Target column" toobe "Col1", and hello "Number of desired features" too50.</span></span> <span data-ttu-id="80d4a-134">Sedan hello modulen [Filter-baserade Funktionsurval] [ filter-based-feature-selection] genererar en datamängd som innehåller 50 funktioner tillsammans med hello målattribut ”Kol1”.</span><span class="sxs-lookup"><span data-stu-id="80d4a-134">Then hello module [Filter-Based Feature Selection][filter-based-feature-selection] will produce a dataset containing 50 features together with hello target attribute "Col1".</span></span> <span data-ttu-id="80d4a-135">hello följande bild visar hello flödet av experimentet och hello indataparametrar vi bara beskrivs.</span><span class="sxs-lookup"><span data-stu-id="80d4a-135">hello following figure shows hello flow of this experiment and hello input parameters we just described.</span></span>

![Funktionen val exempel](./media/machine-learning-data-science-select-features/feature-Selection1.png)

<span data-ttu-id="80d4a-137">hello visar följande bild hello resulterande datauppsättningar.</span><span class="sxs-lookup"><span data-stu-id="80d4a-137">hello following figure shows hello resulting datasets.</span></span> <span data-ttu-id="80d4a-138">Varje funktion beräknas baserat på hello kvadratvärdet korrelation mellan sig själv och hello målattribut ”Kol1”.</span><span class="sxs-lookup"><span data-stu-id="80d4a-138">Each feature is scored based on hello Pearson Correlation between itself and hello target attribute "Col1".</span></span> <span data-ttu-id="80d4a-139">hello funktioner med högsta poäng behålls.</span><span class="sxs-lookup"><span data-stu-id="80d4a-139">hello features with top scores are kept.</span></span>

![Funktionen val exempel](./media/machine-learning-data-science-select-features/feature-Selection2.png)

<span data-ttu-id="80d4a-141">hello motsvarande resultat av hello valda funktioner som visas i hello följande bild.</span><span class="sxs-lookup"><span data-stu-id="80d4a-141">hello corresponding scores of hello selected features are shown in hello following figure.</span></span>

![Funktionen val exempel](./media/machine-learning-data-science-select-features/feature-Selection3.png)

<span data-ttu-id="80d4a-143">Genom att använda detta [Filter-baserade Funktionsurval] [ filter-based-feature-selection] modulen, 50 av 256 funktioner är markerade eftersom de har hello mest korrelerade funktioner med hello målvariabel ”Kol1” baserat på hello bedömningen metoden ”kvadratvärdet korrelation”.</span><span class="sxs-lookup"><span data-stu-id="80d4a-143">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have hello most correlated features with hello target variable "Col1", based on hello scoring method "Pearson Correlation".</span></span>

## <a name="conclusion"></a><span data-ttu-id="80d4a-144">Slutsats</span><span class="sxs-lookup"><span data-stu-id="80d4a-144">Conclusion</span></span>
<span data-ttu-id="80d4a-145">Funktionen tekniker och Funktionsurval är två ofta utformad och de valda funktionerna öka hello effektiviteten för hello utbildning process som försöker tooextract hello nyckelinformation i hello data.</span><span class="sxs-lookup"><span data-stu-id="80d4a-145">Feature engineering and feature selection are two commonly Engineered and selected features increase hello efficiency of hello training process which attempts tooextract hello key information contained in hello data.</span></span> <span data-ttu-id="80d4a-146">De även förbättra hello kraften i indata dessa modeller tooclassify hello korrekt och toopredict resultat för intresse mer robustly.</span><span class="sxs-lookup"><span data-stu-id="80d4a-146">They also improve hello power of these models tooclassify hello input data accurately and toopredict outcomes of interest more robustly.</span></span> <span data-ttu-id="80d4a-147">Funktionen tekniker och val kan också kombinera toomake hello learning mer beräkningsmässigt tractable.</span><span class="sxs-lookup"><span data-stu-id="80d4a-147">Feature engineering and selection can also combine toomake hello learning more computationally tractable.</span></span> <span data-ttu-id="80d4a-148">Den gör detta genom att öka och minska hello antal funktioner behövs toocalibrate eller tåg en modell.</span><span class="sxs-lookup"><span data-stu-id="80d4a-148">It does so by enhancing and then reducing hello number of features needed toocalibrate or train a model.</span></span> <span data-ttu-id="80d4a-149">Matematiskt tala hello funktioner valda tootrain hello modellen är en minimal uppsättning oberoende variabler som förklarar hello mönster i hello data och förutsäga resultat har.</span><span class="sxs-lookup"><span data-stu-id="80d4a-149">Mathematically speaking, hello features selected tootrain hello model are a minimal set of independent variables that explain hello patterns in hello data and then predict outcomes successfully.</span></span>

<span data-ttu-id="80d4a-150">Observera att det inte alltid är tooperform teknik eller funktion val av funktioner.</span><span class="sxs-lookup"><span data-stu-id="80d4a-150">Note that it is not always necessarily tooperform feature engineering or feature selection.</span></span> <span data-ttu-id="80d4a-151">Om det behövs eller inte beror på hello data som vi har eller samla in hello algoritmen vi väljer och hello syftet med hello experiment.</span><span class="sxs-lookup"><span data-stu-id="80d4a-151">Whether it is needed or not depends on hello data we have or collect, hello algorithm we pick, and hello objective of hello experiment.</span></span>

<!-- Module References -->
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/


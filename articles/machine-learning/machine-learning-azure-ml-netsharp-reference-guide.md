---
title: "Guide till Net # Neurala nätverk specifikationsspråk | Microsoft Docs"
description: "Syntaxen för den Net # neurala nätverk specifikationsspråk, tillsammans med exempel på hur du skapar en anpassad neurala nätverket modell i Microsoft Azure ML med hjälp av Net #"
services: machine-learning
documentationcenter: 
author: jeannt
manager: jhubbard
editor: cgronlun
ms.assetid: cfd1454b-47df-4745-b064-ce5f9b3be303
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/31/2017
ms.author: jeannt
ms.openlocfilehash: 965c60ffde55041cc3864d06d81f5590c7ea1c11
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: sv-SE
ms.lasthandoff: 07/11/2017
---
# <a name="guide-to-net-neural-network-specification-language-for-azure-machine-learning"></a><span data-ttu-id="a39da-103">Guide till Net # neurala nätverket specifikationsspråk för Azure Machine Learning</span><span class="sxs-lookup"><span data-stu-id="a39da-103">Guide to Net# neural network specification language for Azure Machine Learning</span></span>
## <a name="overview"></a><span data-ttu-id="a39da-104">Översikt</span><span class="sxs-lookup"><span data-stu-id="a39da-104">Overview</span></span>
<span data-ttu-id="a39da-105">NET # är ett språk som har utvecklats av Microsoft som används för att definiera neural nätverksarkitekturer.</span><span class="sxs-lookup"><span data-stu-id="a39da-105">Net# is a language developed by Microsoft that is used to define neural network architectures.</span></span> <span data-ttu-id="a39da-106">Du kan använda Net # i neurala nätverket moduler i Microsoft Azure Machine Learning.</span><span class="sxs-lookup"><span data-stu-id="a39da-106">You can use Net# in neural network modules in Microsoft Azure Machine Learning.</span></span>

<!-- This function doesn't currentlyappear in the MicrosoftML documentation. If it is added in a future update, we can uncomment this text.

, or in the `rxNeuralNetwork()` function in [MicrosoftML](https://msdn.microsoft.com/microsoft-r/microsoftml/microsoftml). 

-->

<span data-ttu-id="a39da-107">I den här artikeln får du lära dig grundläggande begrepp som behövs för att utveckla en anpassad neurala nätverket:</span><span class="sxs-lookup"><span data-stu-id="a39da-107">In this article, you will learn basic concepts needed to develop a custom neural network:</span></span> 

* <span data-ttu-id="a39da-108">Neural nätverkskrav och hur du definierar de primära komponenterna</span><span class="sxs-lookup"><span data-stu-id="a39da-108">Neural network requirements and how to define the primary components</span></span>
* <span data-ttu-id="a39da-109">Syntax och nyckelord för Net #-specifikationsspråk</span><span class="sxs-lookup"><span data-stu-id="a39da-109">The syntax and keywords of the Net# specification language</span></span>
* <span data-ttu-id="a39da-110">Exempel på anpassade neurala nätverk som skapats med hjälp av Net #</span><span class="sxs-lookup"><span data-stu-id="a39da-110">Examples of custom neural networks created using Net#</span></span> 

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

## <a name="neural-network-basics"></a><span data-ttu-id="a39da-111">Grunderna i neurala nätverket</span><span class="sxs-lookup"><span data-stu-id="a39da-111">Neural network basics</span></span>
<span data-ttu-id="a39da-112">En neurala nätverk som består av ***noder*** som är ordnade i ***lager***, och det vägda ***anslutningar*** (eller ***kanter***) mellan den noder.</span><span class="sxs-lookup"><span data-stu-id="a39da-112">A neural network structure consists of ***nodes*** that are organized in ***layers***, and weighted ***connections*** (or ***edges***) between the nodes.</span></span> <span data-ttu-id="a39da-113">Anslutningarna är riktat och varje anslutning har en ***källa*** nod och en ***mål*** nod.</span><span class="sxs-lookup"><span data-stu-id="a39da-113">The connections are directional, and each connection has a ***source*** node and a ***destination*** node.</span></span>  

<span data-ttu-id="a39da-114">Varje ***trainable layer*** (en dold eller ett lager för utdata) har en eller flera ***anslutning paket***.</span><span class="sxs-lookup"><span data-stu-id="a39da-114">Each ***trainable layer*** (a hidden or an output layer) has one or more ***connection bundles***.</span></span> <span data-ttu-id="a39da-115">En anslutning bunt består av ett lager för källa och en specifikation av anslutningarna från källan lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-115">A connection bundle consists of a source layer and a specification of the connections from that source layer.</span></span> <span data-ttu-id="a39da-116">Alla anslutningar i ett visst paket med samma ***källa layer*** och samma ***mållagret***.</span><span class="sxs-lookup"><span data-stu-id="a39da-116">All the connections in a given bundle share the same ***source layer*** and the same ***destination layer***.</span></span> <span data-ttu-id="a39da-117">I Net # anses en anslutning paket som hör till i bunten mållagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-117">In Net#, a connection bundle is considered as belonging to the bundle's destination layer.</span></span>  

<span data-ttu-id="a39da-118">NET # stöder olika typer av paket, som kan du anpassa hur indata mappas till dolda lager och mappas till utdata.</span><span class="sxs-lookup"><span data-stu-id="a39da-118">Net# supports various kinds of connection bundles, which lets you customize the way inputs are mapped to hidden layers and mapped to the outputs.</span></span>   

<span data-ttu-id="a39da-119">Standard- eller standard paket är en **fullständig paket**, i som varje nod i käll-lagret är ansluten till varje nod i mållagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-119">The default or standard bundle is a **full bundle**, in which each node in the source layer is connected to every node in the destination layer.</span></span>  

<span data-ttu-id="a39da-120">Dessutom stöder Net # följande fyra typer av avancerad anslutning paket:</span><span class="sxs-lookup"><span data-stu-id="a39da-120">Additionally, Net# supports the following four kinds of advanced connection bundles:</span></span>  

* <span data-ttu-id="a39da-121">**Filtrerade paket**.</span><span class="sxs-lookup"><span data-stu-id="a39da-121">**Filtered bundles**.</span></span> <span data-ttu-id="a39da-122">Användaren kan definiera ett predikat för layer Källnoden och lager målnoden.</span><span class="sxs-lookup"><span data-stu-id="a39da-122">The user can define a predicate by using the locations of the source layer node and the destination layer node.</span></span> <span data-ttu-id="a39da-123">Noder är anslutna när predikatet är True.</span><span class="sxs-lookup"><span data-stu-id="a39da-123">Nodes are connected whenever the predicate is True.</span></span>
* <span data-ttu-id="a39da-124">**Convolutional paket**.</span><span class="sxs-lookup"><span data-stu-id="a39da-124">**Convolutional bundles**.</span></span> <span data-ttu-id="a39da-125">Användaren kan definiera små neighborhoods av noder i käll-lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-125">The user can define small neighborhoods of nodes in the source layer.</span></span> <span data-ttu-id="a39da-126">Varje nod i mållagret är ansluten till nätverket som en av noderna i käll-lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-126">Each node in the destination layer is connected to one neighborhood of nodes in the source layer.</span></span>
* <span data-ttu-id="a39da-127">**Poolning paket** och **svar normalisering paket**.</span><span class="sxs-lookup"><span data-stu-id="a39da-127">**Pooling bundles** and **Response normalization bundles**.</span></span> <span data-ttu-id="a39da-128">Dessa liknar convolutional paket i att användaren definierar små neighborhoods av noder i käll-lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-128">These are similar to convolutional bundles in that the user defines small neighborhoods of nodes in the source layer.</span></span> <span data-ttu-id="a39da-129">Skillnaden är att vikten av kanter i dessa paket inte är trainable.</span><span class="sxs-lookup"><span data-stu-id="a39da-129">The difference is that the weights of the edges in these bundles are not trainable.</span></span> <span data-ttu-id="a39da-130">I stället används en fördefinierad funktion på noden källvärden att fastställa värdet för mål-nod.</span><span class="sxs-lookup"><span data-stu-id="a39da-130">Instead, a predefined function is applied to the source node values to determine the destination node value.</span></span>  

<span data-ttu-id="a39da-131">Använda Net # för att definiera strukturen på neurala nätverk gör det möjligt att definiera komplexa datastrukturer som djupa neurala nätverk eller faltningar av godtycklig dimensioner som är kända för att förbättra inlärning på data, till exempel bild, ljud och video.</span><span class="sxs-lookup"><span data-stu-id="a39da-131">Using Net# to define the structure of a neural network makes it possible to define complex structures such as deep neural networks or convolutions of arbitrary dimensions, which are known to improve learning on data such as image, audio, or video.</span></span>  

## <a name="supported-customizations"></a><span data-ttu-id="a39da-132">Anpassningar som stöds</span><span class="sxs-lookup"><span data-stu-id="a39da-132">Supported customizations</span></span>
<span data-ttu-id="a39da-133">Arkitektur för neurala nätverket modeller som du skapar i Azure Machine Learning kan stor utsträckning anpassas med hjälp av Net #.</span><span class="sxs-lookup"><span data-stu-id="a39da-133">The architecture of neural network models that you create in Azure Machine Learning can be extensively customized by using Net#.</span></span> <span data-ttu-id="a39da-134">Du kan:</span><span class="sxs-lookup"><span data-stu-id="a39da-134">You can:</span></span>  

* <span data-ttu-id="a39da-135">Skapa dolda lager och styr antalet noder i varje lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-135">Create hidden layers and control the number of nodes in each layer.</span></span>
* <span data-ttu-id="a39da-136">Ange hur lager ska vara anslutna till varandra.</span><span class="sxs-lookup"><span data-stu-id="a39da-136">Specify how layers are to be connected to each other.</span></span>
* <span data-ttu-id="a39da-137">Definiera särskilda anslutningar, till exempel faltningar och vikt delning paket.</span><span class="sxs-lookup"><span data-stu-id="a39da-137">Define special connectivity structures, such as convolutions and weight sharing bundles.</span></span>
* <span data-ttu-id="a39da-138">Ange olika aktivering funktioner.</span><span class="sxs-lookup"><span data-stu-id="a39da-138">Specify different activation functions.</span></span>  

<span data-ttu-id="a39da-139">Mer information om syntaxen specifikation språk finns [struktur specifikationen](#Structure-specifications).</span><span class="sxs-lookup"><span data-stu-id="a39da-139">For details of the specification language syntax, see [Structure Specification](#Structure-specifications).</span></span>  

<span data-ttu-id="a39da-140">Exempel på definierar neurala nätverk för några vanliga maskininlärning aktiviteter från simplex till komplexa, finns [exempel](#Examples-of-Net#-usage).</span><span class="sxs-lookup"><span data-stu-id="a39da-140">For examples of defining neural networks for some common machine learning tasks, from simplex to complex, see [Examples](#Examples-of-Net#-usage).</span></span>  

## <a name="general-requirements"></a><span data-ttu-id="a39da-141">Allmänna krav</span><span class="sxs-lookup"><span data-stu-id="a39da-141">General requirements</span></span>
* <span data-ttu-id="a39da-142">Det måste finnas exakt en utgående lager, minst ett indata lager och noll eller flera dolda lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-142">There must be exactly one output layer, at least one input layer, and zero or more hidden layers.</span></span> 
* <span data-ttu-id="a39da-143">Varje lager har ett fast antal noder, begreppsmässigt ordnade i en rektangulär matris av godtycklig dimensioner.</span><span class="sxs-lookup"><span data-stu-id="a39da-143">Each layer has a fixed number of nodes, conceptually arranged in a rectangular array of arbitrary dimensions.</span></span> 
* <span data-ttu-id="a39da-144">Inkommande lager saknar associerade utbildade parametrar som representerar den punkt där instansens data kommer in i nätverket.</span><span class="sxs-lookup"><span data-stu-id="a39da-144">Input layers have no associated trained parameters and represent the point where instance data enters the network.</span></span> 
* <span data-ttu-id="a39da-145">Trainable lager (dolda och utdata lager) har associerade utbildade parametrar, kallas även eventuella fördomar och vikt.</span><span class="sxs-lookup"><span data-stu-id="a39da-145">Trainable layers (the hidden and output layers) have associated trained parameters, known as weights and biases.</span></span> 
* <span data-ttu-id="a39da-146">Käll- och noder måste vara i separata lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-146">The source and destination nodes must be in separate layers.</span></span> 
* <span data-ttu-id="a39da-147">Anslutningar måste vara acykliska; med andra ord kan det finnas en kedja av anslutningar som leder tillbaka till den ursprungliga noden.</span><span class="sxs-lookup"><span data-stu-id="a39da-147">Connections must be acyclic; in other words, there cannot be a chain of connections leading back to the initial source node.</span></span>
* <span data-ttu-id="a39da-148">Lagret utdata kan inte vara ett källa lager för ett paket för anslutningen.</span><span class="sxs-lookup"><span data-stu-id="a39da-148">The output layer cannot be a source layer of a connection bundle.</span></span>  

## <a name="structure-specifications"></a><span data-ttu-id="a39da-149">Specifikationer för struktur</span><span class="sxs-lookup"><span data-stu-id="a39da-149">Structure specifications</span></span>
<span data-ttu-id="a39da-150">En neurala nätverket struktur specifikation består av tre delar: den **konstantdeklaration**, **layer deklaration**, **anslutning deklaration**.</span><span class="sxs-lookup"><span data-stu-id="a39da-150">A neural network structure specification is composed of three sections: the **constant declaration**, the **layer declaration**, the **connection declaration**.</span></span> <span data-ttu-id="a39da-151">Det finns även en valfri **dela deklaration** avsnitt.</span><span class="sxs-lookup"><span data-stu-id="a39da-151">There is also an optional **share declaration** section.</span></span> <span data-ttu-id="a39da-152">Avsnitten kan anges i valfri ordning.</span><span class="sxs-lookup"><span data-stu-id="a39da-152">The sections can be specified in any order.</span></span>  

## <a name="constant-declaration"></a><span data-ttu-id="a39da-153">Konstantdeklaration</span><span class="sxs-lookup"><span data-stu-id="a39da-153">Constant declaration</span></span>
<span data-ttu-id="a39da-154">En konstantdeklaration är valfritt.</span><span class="sxs-lookup"><span data-stu-id="a39da-154">A constant declaration is optional.</span></span> <span data-ttu-id="a39da-155">Det ger dig möjlighet att definiera värden används någon annanstans i definitionen av neurala nätverket.</span><span class="sxs-lookup"><span data-stu-id="a39da-155">It provides a means to define values used elsewhere in the neural network definition.</span></span> <span data-ttu-id="a39da-156">Instruktionen deklaration består av en identifierare som följt av ett likhetstecken och ett värdeuttryck.</span><span class="sxs-lookup"><span data-stu-id="a39da-156">The declaration statement consists of an identifier followed by an equal sign and a value expression.</span></span>   

<span data-ttu-id="a39da-157">Till exempel följande sats definierar en konstant **x**:</span><span class="sxs-lookup"><span data-stu-id="a39da-157">For example, the following statement defines a constant **x**:</span></span>  

    Const X = 28;  

<span data-ttu-id="a39da-158">Om du vill definiera två eller flera konstanter samtidigt omges klammerparenteser identifierarnamn och värden och avgränsa dem med semikolon.</span><span class="sxs-lookup"><span data-stu-id="a39da-158">To define two or more constants simultaneously, enclose the identifier names and values in braces, and separate them by using semicolons.</span></span> <span data-ttu-id="a39da-159">Exempel:</span><span class="sxs-lookup"><span data-stu-id="a39da-159">For example:</span></span>  

    Const { X = 28; Y = 4; }  

<span data-ttu-id="a39da-160">Till höger i varje tilldelning uttrycket kan vara ett heltal, ett verkligt tal, ett booleskt värde (True eller False) eller ett matematiska uttryck.</span><span class="sxs-lookup"><span data-stu-id="a39da-160">The right-hand side of each assignment expression can be an integer, a real number, a Boolean value (True or False), or a mathematical expression.</span></span> <span data-ttu-id="a39da-161">Exempel:</span><span class="sxs-lookup"><span data-stu-id="a39da-161">For example:</span></span>  

    Const { X = 17 * 2; Y = true; }  

## <a name="layer-declaration"></a><span data-ttu-id="a39da-162">Deklarationen för lager</span><span class="sxs-lookup"><span data-stu-id="a39da-162">Layer declaration</span></span>
<span data-ttu-id="a39da-163">Layer-deklaration krävs.</span><span class="sxs-lookup"><span data-stu-id="a39da-163">The layer declaration is required.</span></span> <span data-ttu-id="a39da-164">Den definierar storlek och källa för lager, inklusive dess anslutning paket och attribut.</span><span class="sxs-lookup"><span data-stu-id="a39da-164">It defines the size and source of the layer, including its connection bundles and attributes.</span></span> <span data-ttu-id="a39da-165">Instruktionen deklaration börjar med namnet på lagret (indata, dolda eller utdata) följt av dimensionerna lagret (en tuppel positiva heltal).</span><span class="sxs-lookup"><span data-stu-id="a39da-165">The declaration statement starts with the name of the layer (input, hidden, or output), followed by the dimensions of the layer (a tuple of positive integers).</span></span> <span data-ttu-id="a39da-166">Exempel:</span><span class="sxs-lookup"><span data-stu-id="a39da-166">For example:</span></span>  

    input Data auto;
    hidden Hidden[5,20] from Data all;
    output Result[2] from Hidden all;  

* <span data-ttu-id="a39da-167">Produkten av dimensionerna är antalet noder i lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-167">The product of the dimensions is the number of nodes in the layer.</span></span> <span data-ttu-id="a39da-168">I det här exemplet finns det två dimensioner [5,20], vilket innebär att det finns 100 noder i lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-168">In this example, there are two dimensions [5,20], which means there are  100 nodes in the layer.</span></span>
* <span data-ttu-id="a39da-169">Lagren kan deklareras i vilken ordning som helst, med ett undantag: om fler än ett inkommande lager definieras den ordning de deklareras måste matcha ordningen på funktioner i indata.</span><span class="sxs-lookup"><span data-stu-id="a39da-169">The layers can be declared in any order, with one exception: If more than one input layer is defined, the order in which they are declared must match the order of features in the input data.</span></span>  

<span data-ttu-id="a39da-170">Ange antalet noder i ett lager fastställas automatiskt den **automatisk** nyckelord.</span><span class="sxs-lookup"><span data-stu-id="a39da-170">To specify that the number of nodes in a layer be determined automatically, use the **auto** keyword.</span></span> <span data-ttu-id="a39da-171">Den **automatisk** nyckelordet har olika effekter, beroende på lagret:</span><span class="sxs-lookup"><span data-stu-id="a39da-171">The **auto** keyword has different effects, depending on the layer:</span></span>  

* <span data-ttu-id="a39da-172">I en inkommande layer-deklaration är antalet noder antal funktioner i indata.</span><span class="sxs-lookup"><span data-stu-id="a39da-172">In an input layer declaration, the number of nodes is the number of features in the input data.</span></span>
* <span data-ttu-id="a39da-173">I en deklaration som dolda lagret antalet noder är det tal som anges av parametervärdet för **antalet dolda noder**.</span><span class="sxs-lookup"><span data-stu-id="a39da-173">In a hidden layer declaration, the number of nodes is the number that is specified by the parameter value for **Number of hidden nodes**.</span></span> 
* <span data-ttu-id="a39da-174">I en layer deklaration utdata är antalet noder 2 för tvåklass klassificering, 1 för regression och som är lika med antalet noder som utdata för multiklass-baserad klassificering.</span><span class="sxs-lookup"><span data-stu-id="a39da-174">In an output layer declaration, the number of nodes is 2 for two-class classification, 1 for regression, and equal to the number of output nodes for multiclass classification.</span></span>   

<span data-ttu-id="a39da-175">Följande nätverksdefinitionen kan till exempel storleken på alla skikt bestäms automatiskt:</span><span class="sxs-lookup"><span data-stu-id="a39da-175">For example, the following network definition allows the size of all layers to be automatically determined:</span></span>  

    input Data auto;
    hidden Hidden auto from Data all;
    output Result auto from Hidden all;  


<span data-ttu-id="a39da-176">En layer-deklaration för en trainable lager (dolda eller utdata-lager) kan du också inkludera utdata funktionen (kallas även en funktion för aktivering), där standardinställningen är **sigmoid** för klassificering modeller och  **linjär** för regression modeller.</span><span class="sxs-lookup"><span data-stu-id="a39da-176">A layer declaration for a trainable layer (the hidden or output layers) can optionally include the output function (also called an activation function), which defaults to **sigmoid** for classification models, and **linear** for regression models.</span></span> <span data-ttu-id="a39da-177">(Även om du använder standard du uttryckligen uppge funktionen aktivering, om så önskas för tydlighetens skull.)</span><span class="sxs-lookup"><span data-stu-id="a39da-177">(Even if you use the default, you can explicitly state the activation function, if desired for clarity.)</span></span>

<span data-ttu-id="a39da-178">Följande utdata-funktioner stöds:</span><span class="sxs-lookup"><span data-stu-id="a39da-178">The following output functions are supported:</span></span>  

* <span data-ttu-id="a39da-179">sigmoid</span><span class="sxs-lookup"><span data-stu-id="a39da-179">sigmoid</span></span>
* <span data-ttu-id="a39da-180">linjär</span><span class="sxs-lookup"><span data-stu-id="a39da-180">linear</span></span>
* <span data-ttu-id="a39da-181">softmax</span><span class="sxs-lookup"><span data-stu-id="a39da-181">softmax</span></span>
* <span data-ttu-id="a39da-182">rlinear</span><span class="sxs-lookup"><span data-stu-id="a39da-182">rlinear</span></span>
* <span data-ttu-id="a39da-183">Ruta</span><span class="sxs-lookup"><span data-stu-id="a39da-183">square</span></span>
* <span data-ttu-id="a39da-184">rot</span><span class="sxs-lookup"><span data-stu-id="a39da-184">sqrt</span></span>
* <span data-ttu-id="a39da-185">srlinear</span><span class="sxs-lookup"><span data-stu-id="a39da-185">srlinear</span></span>
* <span data-ttu-id="a39da-186">ABS</span><span class="sxs-lookup"><span data-stu-id="a39da-186">abs</span></span>
* <span data-ttu-id="a39da-187">TANH</span><span class="sxs-lookup"><span data-stu-id="a39da-187">tanh</span></span> 
* <span data-ttu-id="a39da-188">brlinear</span><span class="sxs-lookup"><span data-stu-id="a39da-188">brlinear</span></span>  

<span data-ttu-id="a39da-189">Till exempel följande deklaration använder den **softmax** funktionen:</span><span class="sxs-lookup"><span data-stu-id="a39da-189">For example, the following declaration uses the **softmax** function:</span></span>  

    output Result [100] softmax from Hidden all;  

## <a name="connection-declaration"></a><span data-ttu-id="a39da-190">Deklarationen för anslutning</span><span class="sxs-lookup"><span data-stu-id="a39da-190">Connection declaration</span></span>
<span data-ttu-id="a39da-191">Du måste deklarera anslutningar mellan lager som du har definierat omedelbart när du har definierat trainable lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-191">Immediately after defining the trainable layer, you must declare connections among the layers you have defined.</span></span> <span data-ttu-id="a39da-192">Anslutningen paket deklarationen börjar med nyckelordet **från**, följt av namnet på det paket källa lager och vilken typ av anslutning paket för att skapa.</span><span class="sxs-lookup"><span data-stu-id="a39da-192">The connection bundle declaration starts with the keyword **from**, followed by the name of the bundle's source layer and the kind of connection bundle to create.</span></span>   

<span data-ttu-id="a39da-193">För närvarande stöds fem typer av paket för anslutningen:</span><span class="sxs-lookup"><span data-stu-id="a39da-193">Currently, five kinds of connection bundles are supported:</span></span>  

* <span data-ttu-id="a39da-194">**Fullständig** paket som anges av nyckelordet **alla**</span><span class="sxs-lookup"><span data-stu-id="a39da-194">**Full** bundles, indicated by the keyword **all**</span></span>
* <span data-ttu-id="a39da-195">**Filtrerade** paket som anges av nyckelordet **där**, följt av ett predikat uttryck</span><span class="sxs-lookup"><span data-stu-id="a39da-195">**Filtered** bundles, indicated by the keyword **where**, followed by a predicate expression</span></span>
* <span data-ttu-id="a39da-196">**Convolutional** paket som anges av nyckelordet **convolve**, följt av Faltning attribut</span><span class="sxs-lookup"><span data-stu-id="a39da-196">**Convolutional** bundles, indicated by the keyword **convolve**, followed by the convolution attributes</span></span>
* <span data-ttu-id="a39da-197">**Poolning** paket som anges av nyckelorden **max poolen** eller **innebär pool**</span><span class="sxs-lookup"><span data-stu-id="a39da-197">**Pooling** bundles, indicated by the keywords **max pool** or **mean pool**</span></span>
* <span data-ttu-id="a39da-198">**Svaret normalisering** paket som anges av nyckelordet **svar normen**</span><span class="sxs-lookup"><span data-stu-id="a39da-198">**Response normalization** bundles, indicated by the keyword **response norm**</span></span>      

## <a name="full-bundles"></a><span data-ttu-id="a39da-199">Fullständig paket</span><span class="sxs-lookup"><span data-stu-id="a39da-199">Full bundles</span></span>
<span data-ttu-id="a39da-200">En fullständig anslutning bunt innehåller en anslutning från varje nod i lagret för källan till varje nod i mållagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-200">A full connection bundle includes a connection from each node in the source layer to each node in the destination layer.</span></span> <span data-ttu-id="a39da-201">Detta är typ av nätverksanslutning standard.</span><span class="sxs-lookup"><span data-stu-id="a39da-201">This is the default network connection type.</span></span>  

## <a name="filtered-bundles"></a><span data-ttu-id="a39da-202">Filtrerade paket</span><span class="sxs-lookup"><span data-stu-id="a39da-202">Filtered bundles</span></span>
<span data-ttu-id="a39da-203">En filtrerad anslutning paket specifikation innehåller ett predikat, uttryckt syntaktiskt, mycket som ett C# lambda-uttryck.</span><span class="sxs-lookup"><span data-stu-id="a39da-203">A filtered connection bundle specification includes a predicate, expressed syntactically, much like a C# lambda expression.</span></span> <span data-ttu-id="a39da-204">I följande exempel definierar två filtrerade paket:</span><span class="sxs-lookup"><span data-stu-id="a39da-204">The following example defines two filtered bundles:</span></span>  

    input Pixels [10, 20];
    hidden ByRow[10, 12] from Pixels where (s,d) => s[0] == d[0];
    hidden ByCol[5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;  

* <span data-ttu-id="a39da-205">I predikatet för *ByRow*, **s** är en parameter som representerar ett index i rektangulär matrisen noder i det inkommande lagret *bildpunkter*, och **d** är en parameter som representerar ett index i matrisen noder i det dolda lagret *ByRow*.</span><span class="sxs-lookup"><span data-stu-id="a39da-205">In the predicate for *ByRow*, **s** is a parameter representing an index into the rectangular array of nodes of the input layer, *Pixels*, and **d** is a parameter representing an index into the array of nodes of the hidden layer, *ByRow*.</span></span> <span data-ttu-id="a39da-206">Typ av både **s** och **d** är en tuppel med heltal med längden två.</span><span class="sxs-lookup"><span data-stu-id="a39da-206">The type of both **s** and **d** is a tuple of integers of length two.</span></span> <span data-ttu-id="a39da-207">Begreppsmässigt **s** sträcker sig över alla par med heltal med *0 < = s [0] < 10* och *0 < = s[1] < 20*, och **d**  sträcker sig över alla par med heltal, med *0 < = d [0] < 10* och *0 < = d[1] < 12*.</span><span class="sxs-lookup"><span data-stu-id="a39da-207">Conceptually, **s** ranges over all pairs of integers with *0 <= s[0] < 10* and *0 <= s[1] < 20*, and **d** ranges over all pairs of integers, with *0 <= d[0] < 10* and *0 <= d[1] < 12*.</span></span> 
* <span data-ttu-id="a39da-208">Det finns ett villkor på höger sida av predikatuttrycket.</span><span class="sxs-lookup"><span data-stu-id="a39da-208">On the right-hand side of the predicate expression, there is a condition.</span></span> <span data-ttu-id="a39da-209">I det här exemplet för varje värde i **s** och **d** så att villkoret är sant, det finns en kant från lager Källnoden till målnoden för lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-209">In this example, for every value of **s** and **d** such that the condition is True, there is an edge from the source layer node to the destination layer node.</span></span> <span data-ttu-id="a39da-210">Därför filteruttrycket anger att paketet innehåller en anslutning från den nod som definierats av **s** till den nod som definierats av **d** i samtliga fall där s [0] är lika med d [0].</span><span class="sxs-lookup"><span data-stu-id="a39da-210">Thus, this filter expression indicates that the bundle includes a connection from the node defined by **s** to the node defined by **d** in all cases where s[0] is equal to d[0].</span></span>  

<span data-ttu-id="a39da-211">Du kan också kan du ange en uppsättning vikterna för ett filtrerade paket.</span><span class="sxs-lookup"><span data-stu-id="a39da-211">Optionally, you can specify a set of weights for a filtered bundle.</span></span> <span data-ttu-id="a39da-212">Värdet för den **vikterna** attributet måste vara en tuppel av flytande punktvärden med en längd som stämmer med antalet anslutningar som definieras av paketet.</span><span class="sxs-lookup"><span data-stu-id="a39da-212">The value for the **Weights** attribute must be a tuple of floating point values with a length that matches the number of connections defined by the bundle.</span></span> <span data-ttu-id="a39da-213">Som standard genereras slumpmässigt vikter.</span><span class="sxs-lookup"><span data-stu-id="a39da-213">By default, weights are randomly generated.</span></span>  

<span data-ttu-id="a39da-214">Värde som är grupperade efter nodindex mål.</span><span class="sxs-lookup"><span data-stu-id="a39da-214">Weight values are grouped by the destination node index.</span></span> <span data-ttu-id="a39da-215">Som, om den första Målnoden är ansluten till K källa noder första *K* element i den **vikter** tuppel är vikterna för första målnoden i indexet källordning.</span><span class="sxs-lookup"><span data-stu-id="a39da-215">That is, if the first destination node is connected to K source nodes, the first *K* elements of the **Weights** tuple are the weights for the first destination node, in source index order.</span></span> <span data-ttu-id="a39da-216">Detsamma gäller för de återstående noderna i målet.</span><span class="sxs-lookup"><span data-stu-id="a39da-216">The same applies for the remaining destination nodes.</span></span>  

<span data-ttu-id="a39da-217">Det är möjligt att ange vikterna direkt som konstanta värden.</span><span class="sxs-lookup"><span data-stu-id="a39da-217">It's possible to specify weights directly as constant values.</span></span> <span data-ttu-id="a39da-218">Om du har lärt dig vikterna tidigare kan du exempelvis ange dem som konstanter med följande syntax:</span><span class="sxs-lookup"><span data-stu-id="a39da-218">For example, if you learned the weights previously, you can specify them as constants using this syntax:</span></span>

    const Weights_1 = [0.0188045055, 0.130500451, ...]


## <a name="convolutional-bundles"></a><span data-ttu-id="a39da-219">Convolutional paket</span><span class="sxs-lookup"><span data-stu-id="a39da-219">Convolutional bundles</span></span>
<span data-ttu-id="a39da-220">När av att träningsinformationen har en homogen struktur, används convolutional anslutningar ofta mer avancerade funktioner för data.</span><span class="sxs-lookup"><span data-stu-id="a39da-220">When the training data has a homogeneous structure, convolutional connections are commonly used to learn high-level features of the data.</span></span> <span data-ttu-id="a39da-221">Till exempel i bild, ljud- och data, spatial eller temporal dimensionalitet kan vara ganska enhetlig.</span><span class="sxs-lookup"><span data-stu-id="a39da-221">For example, in image, audio, or video data, spatial or temporal dimensionality can be fairly uniform.</span></span>  

<span data-ttu-id="a39da-222">Convolutional paket använder rektangulär **kärnor** som slid via dimensionerna.</span><span class="sxs-lookup"><span data-stu-id="a39da-222">Convolutional bundles employ rectangular **kernels** that are slid through the dimensions.</span></span> <span data-ttu-id="a39da-223">I princip varje kernel definierar en uppsättning vikten som används i lokala neighborhoods kallas **kernel-program**.</span><span class="sxs-lookup"><span data-stu-id="a39da-223">Essentially, each kernel defines a set of weights applied in local neighborhoods, referred to as **kernel applications**.</span></span> <span data-ttu-id="a39da-224">Varje kernel-program som motsvarar en nod i lagret källa, som kallas den **centrala nod**.</span><span class="sxs-lookup"><span data-stu-id="a39da-224">Each kernel application corresponds to a node in the source layer, which is referred to as the **central node**.</span></span> <span data-ttu-id="a39da-225">Vikten av en kernel delas av många anslutningar.</span><span class="sxs-lookup"><span data-stu-id="a39da-225">The weights of a kernel are shared among many connections.</span></span> <span data-ttu-id="a39da-226">I ett convolutional paket varje kernel är rektangulär och alla kernel-program har samma storlek.</span><span class="sxs-lookup"><span data-stu-id="a39da-226">In a convolutional bundle, each kernel is rectangular and all kernel applications are the same size.</span></span>  

<span data-ttu-id="a39da-227">Convolutional paket har stöd för följande attribut:</span><span class="sxs-lookup"><span data-stu-id="a39da-227">Convolutional bundles support the following attributes:</span></span>

<span data-ttu-id="a39da-228">**InputShape** definierar dimensionalitet skiktets källa för den här convolutional paket.</span><span class="sxs-lookup"><span data-stu-id="a39da-228">**InputShape** defines the dimensionality of the source layer for the purposes of this convolutional bundle.</span></span> <span data-ttu-id="a39da-229">Värdet måste vara en tuppel med positivt heltal.</span><span class="sxs-lookup"><span data-stu-id="a39da-229">The value must be a tuple of positive integers.</span></span> <span data-ttu-id="a39da-230">Produkten av heltalen måste vara lika med antalet noder i lagret källan, men i annat fall det behöver inte matchar dimensionaliteten har deklarerats för käll-lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-230">The product of the integers must equal the number of nodes in the source layer, but otherwise, it does not need to match the dimensionality declared for the source layer.</span></span> <span data-ttu-id="a39da-231">Längden på den här tuppeln blir den **aritet** värde för convolutional paket.</span><span class="sxs-lookup"><span data-stu-id="a39da-231">The length of this tuple becomes the **arity** value for the convolutional bundle.</span></span> <span data-ttu-id="a39da-232">(Normalt aritet avser antalet argument eller operander som en funktion kan vidta.)</span><span class="sxs-lookup"><span data-stu-id="a39da-232">(Typically arity refers to the number of arguments or operands that a function can take.)</span></span>  

<span data-ttu-id="a39da-233">Använda attribut för att definiera form och platserna för kärnor, **KernelShape**, **Stride**, **utfyllnad**, **LowerPad**, och  **UpperPad**:</span><span class="sxs-lookup"><span data-stu-id="a39da-233">To define the shape and locations of the kernels, use the attributes **KernelShape**, **Stride**, **Padding**, **LowerPad**, and **UpperPad**:</span></span>   

* <span data-ttu-id="a39da-234">**KernelShape**: (obligatoriskt) anger dimensionaliteten för varje kernel för convolutional paket.</span><span class="sxs-lookup"><span data-stu-id="a39da-234">**KernelShape**: (required) Defines the dimensionality of each kernel for the convolutional bundle.</span></span> <span data-ttu-id="a39da-235">Värdet måste vara en tuppel med positiva heltal med en längd som är lika med ariteten för paketet.</span><span class="sxs-lookup"><span data-stu-id="a39da-235">The value must be a tuple of positive integers with a length that equals the arity of the bundle.</span></span> <span data-ttu-id="a39da-236">Varje komponent i den här tuppeln får inte vara större än den motsvarande komponenten i **InputShape**.</span><span class="sxs-lookup"><span data-stu-id="a39da-236">Each component of this tuple must be no greater than the corresponding component of **InputShape**.</span></span> 
* <span data-ttu-id="a39da-237">**STRIDE**: (valfritt) definierar glidande steg storlekarna på Faltning (ett Stegstorlek för varje dimension), som är avståndet mellan de centrala noderna.</span><span class="sxs-lookup"><span data-stu-id="a39da-237">**Stride**: (optional) Defines the sliding step sizes of the convolution (one step size for each dimension), that is the distance between the central nodes.</span></span> <span data-ttu-id="a39da-238">Värdet måste vara en tuppel med positiva heltal med en längd som är ariteten för paketet.</span><span class="sxs-lookup"><span data-stu-id="a39da-238">The value must be a tuple of positive integers with a length that is the arity of the bundle.</span></span> <span data-ttu-id="a39da-239">Varje komponent i den här tuppeln får inte vara större än den motsvarande komponenten i **KernelShape**.</span><span class="sxs-lookup"><span data-stu-id="a39da-239">Each component of this tuple must be no greater than the corresponding component of **KernelShape**.</span></span> <span data-ttu-id="a39da-240">Standardvärdet är en tuppel med alla komponenter som är lika med ett.</span><span class="sxs-lookup"><span data-stu-id="a39da-240">The default value is a tuple with all components equal to one.</span></span> 
* <span data-ttu-id="a39da-241">**Dela**: (valfritt) definierar vikten för varje dimension i Faltning.</span><span class="sxs-lookup"><span data-stu-id="a39da-241">**Sharing**: (optional) Defines the weight sharing for each dimension of the convolution.</span></span> <span data-ttu-id="a39da-242">Värdet kan vara ett booleskt värde eller en tuppel med booleska värden med en längd som är ariteten för paketet.</span><span class="sxs-lookup"><span data-stu-id="a39da-242">The value can be a single Boolean value or a tuple of Boolean values with a length that is the arity of the bundle.</span></span> <span data-ttu-id="a39da-243">Ett booleskt värde har utökats för att vara en tuppel rätt längd med alla komponenter som är lika med det angivna värdet.</span><span class="sxs-lookup"><span data-stu-id="a39da-243">A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</span></span> <span data-ttu-id="a39da-244">Standardvärdet är en tuppel som består av alla värden som True.</span><span class="sxs-lookup"><span data-stu-id="a39da-244">The default value is a tuple that consists of all True values.</span></span> 
* <span data-ttu-id="a39da-245">**MapCount**: (valfritt) anger antalet funktionen mappar för convolutional paket.</span><span class="sxs-lookup"><span data-stu-id="a39da-245">**MapCount**: (optional) Defines the number of feature maps for the convolutional bundle.</span></span> <span data-ttu-id="a39da-246">Värdet kan vara ett enda positivt heltal eller en tuppel med positiva heltal med en längd som är ariteten för paketet.</span><span class="sxs-lookup"><span data-stu-id="a39da-246">The value can be a single positive integer or a tuple of positive integers with a length that is the arity of the bundle.</span></span> <span data-ttu-id="a39da-247">Ett enda heltal har utökats för att vara en tuppel med de första komponenterna som är lika med det angivna värdet rätt längd och alla återstående komponenter som är lika med ett.</span><span class="sxs-lookup"><span data-stu-id="a39da-247">A single integer value is extended to be a tuple of the correct length with the first components equal to the specified value and all the remaining components equal to one.</span></span> <span data-ttu-id="a39da-248">Standardvärdet är en.</span><span class="sxs-lookup"><span data-stu-id="a39da-248">The default value is one.</span></span> <span data-ttu-id="a39da-249">Det totala antalet funktionen maps är produkten av komponenterna i tuppeln.</span><span class="sxs-lookup"><span data-stu-id="a39da-249">The total number of feature maps is the product of the components of the tuple.</span></span> <span data-ttu-id="a39da-250">Factoring detta totala antal över komponenterna som bestämmer hur funktionen mappa värden grupperas i mål-noder.</span><span class="sxs-lookup"><span data-stu-id="a39da-250">The factoring of this total number across the components determines how the feature map values are grouped in the destination nodes.</span></span> 
* <span data-ttu-id="a39da-251">**Vikterna**: (valfritt) definierar inledande vikterna för paketet.</span><span class="sxs-lookup"><span data-stu-id="a39da-251">**Weights**: (optional) Defines the initial weights for the bundle.</span></span> <span data-ttu-id="a39da-252">Värdet måste vara en tuppel av flytande punktvärden med en längd som är antalet kärnor gånger antalet vikter per kernel, enligt nedan.</span><span class="sxs-lookup"><span data-stu-id="a39da-252">The value must be a tuple of floating point values with a length that is the number of kernels times the number of weights per kernel, as defined later in this article.</span></span> <span data-ttu-id="a39da-253">Standardvikterna genereras slumpmässigt.</span><span class="sxs-lookup"><span data-stu-id="a39da-253">The default weights are randomly generated.</span></span>  

<span data-ttu-id="a39da-254">Det finns två uppsättningar med egenskaper som styr utfyllnad, egenskaper som inte anges samtidigt:</span><span class="sxs-lookup"><span data-stu-id="a39da-254">There are two sets of properties that control padding, the properties being mutually exclusive:</span></span>

* <span data-ttu-id="a39da-255">**Utfyllnad**: (valfritt) avgör om indata ska bli utfyllt med hjälp av en **utfyllnad standardschema**.</span><span class="sxs-lookup"><span data-stu-id="a39da-255">**Padding**: (optional) Determines whether the input should be padded by using a **default padding scheme**.</span></span> <span data-ttu-id="a39da-256">Värdet kan vara ett booleskt värde eller så kan vara en tuppel med booleska värden med en längd som är ariteten för paketet.</span><span class="sxs-lookup"><span data-stu-id="a39da-256">The value can be a single Boolean value, or it can be a tuple of Boolean values with a length that is the arity of the bundle.</span></span> <span data-ttu-id="a39da-257">Ett booleskt värde har utökats för att vara en tuppel rätt längd med alla komponenter som är lika med det angivna värdet.</span><span class="sxs-lookup"><span data-stu-id="a39da-257">A single Boolean value is extended to be a tuple of the correct length with all components equal to the specified value.</span></span> <span data-ttu-id="a39da-258">Om värdet för en dimension är True, utfyllnad källan logiskt i dimensionen med cellerna noll-värden till ytterligare kernel-program så att centrala noderna i de första och sista kärnor i den aktuella dimensionen är de första och sista noderna i som dimensionen i käll-lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-258">If the value for a dimension is True, the source is logically padded in that dimension with zero-valued cells to support additional kernel applications, such that the central nodes of the first and last kernels in that dimension are the first and last nodes in that dimension in the source layer.</span></span> <span data-ttu-id="a39da-259">Därför antalet ”falska” noder i varje dimension bestäms automatiskt, så att den passar exakt *(InputShape [d] - 1) / Stride [d] + 1* kärnor i lagret Vadderat källa.</span><span class="sxs-lookup"><span data-stu-id="a39da-259">Thus, the number of "dummy" nodes in each dimension is determined automatically, to fit exactly *(InputShape[d] - 1) / Stride[d] + 1* kernels into the padded source layer.</span></span> <span data-ttu-id="a39da-260">Om värdet för en dimension är False, definieras de kernlar som är så att antalet noder på varje sida som lämnas ut är samma (upp till en skillnad på 1).</span><span class="sxs-lookup"><span data-stu-id="a39da-260">If the value for a dimension is False, the kernels are defined so that the number of nodes on each side that are left out is the same (up to a difference of 1).</span></span> <span data-ttu-id="a39da-261">Standardvärdet för det här attributet är en tuppel med alla komponenter som är lika med False.</span><span class="sxs-lookup"><span data-stu-id="a39da-261">The default value of this attribute is a tuple with all components equal to False.</span></span>
* <span data-ttu-id="a39da-262">**UpperPad** och **LowerPad**: (valfritt) Ange större kontroll över utfyllnad ska användas.</span><span class="sxs-lookup"><span data-stu-id="a39da-262">**UpperPad** and **LowerPad**: (optional) Provide greater control over the amount of padding to use.</span></span> <span data-ttu-id="a39da-263">**Viktigt:** attributen kan vara definierade om och bara om den **utfyllnad** egenskapen ovan är ***inte*** definieras.</span><span class="sxs-lookup"><span data-stu-id="a39da-263">**Important:** These attributes can be defined if and only if the **Padding** property above is ***not*** defined.</span></span> <span data-ttu-id="a39da-264">Värdena måste vara heltal enkelvärdesattribut tupplar med längd som är ariteten för paketet.</span><span class="sxs-lookup"><span data-stu-id="a39da-264">The values should be integer-valued tuples with lengths that are the arity of the bundle.</span></span> <span data-ttu-id="a39da-265">När dessa attribut har angetts läggs ”falska” noder till lägre och övre parterna för varje inkommande lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-265">When these attributes are specified, "dummy" nodes are added to the lower and upper ends of each dimension of the input layer.</span></span> <span data-ttu-id="a39da-266">Antalet noder som lagts till i de nedre och övre ends i varje dimension bestäms av **LowerPad**[i] och **UpperPad**[i] respektive.</span><span class="sxs-lookup"><span data-stu-id="a39da-266">The number of nodes added to the lower and upper ends in each dimension is determined by **LowerPad**[i] and **UpperPad**[i] respectively.</span></span> <span data-ttu-id="a39da-267">För att säkerställa att kärnor överensstämmer endast ”verkliga” noder och inte ”falska” noder, måste följande villkor uppfyllas:</span><span class="sxs-lookup"><span data-stu-id="a39da-267">To ensure that kernels correspond only to "real" nodes and not to "dummy" nodes, the following conditions must be met:</span></span>
  * <span data-ttu-id="a39da-268">Varje komponent i **LowerPad** måste vara absolut mindre än KernelShape [d] / 2.</span><span class="sxs-lookup"><span data-stu-id="a39da-268">Each component of **LowerPad** must be strictly less than KernelShape[d]/2.</span></span> 
  * <span data-ttu-id="a39da-269">Varje komponent i **UpperPad** får inte vara större än KernelShape [d] / 2.</span><span class="sxs-lookup"><span data-stu-id="a39da-269">Each component of **UpperPad** must be no greater than KernelShape[d]/2.</span></span> 
  * <span data-ttu-id="a39da-270">Standardvärdet för dessa attribut är en tuppel med alla komponenter som är lika med 0.</span><span class="sxs-lookup"><span data-stu-id="a39da-270">The default value of these attributes is a tuple with all components equal to 0.</span></span> 

<span data-ttu-id="a39da-271">Inställningen **utfyllnad** = true tillåter så mycket utfyllnad som behövs för att hålla ”mittpunkt” kernel inuti den ”verkligt” indata.</span><span class="sxs-lookup"><span data-stu-id="a39da-271">The setting **Padding** = true allows as much padding as is needed to keep the "center" of the kernel inside the "real" input.</span></span> <span data-ttu-id="a39da-272">Math lite för datoranvändning storlek ändras.</span><span class="sxs-lookup"><span data-stu-id="a39da-272">This changes the math a bit for computing the output size.</span></span> <span data-ttu-id="a39da-273">I allmänhet storlek *D* beräknas som *D = (I - K) / S + 1*, där *jag* är inkommande storlek *K* kernel-storlek *S*  är stride, och  */*  är Heltalsdivision (avrunda mot noll).</span><span class="sxs-lookup"><span data-stu-id="a39da-273">Generally, the output size *D* is computed as *D = (I - K) / S + 1*, where *I* is the input size, *K* is the kernel size, *S* is the stride, and */* is integer division (round toward zero).</span></span> <span data-ttu-id="a39da-274">Om du ställer in UpperPad = [1, 1] inkommande storleken *jag* är i praktiken 29, och därmed *D = (29-5) / 2 + 1 = 13*.</span><span class="sxs-lookup"><span data-stu-id="a39da-274">If you set UpperPad = [1, 1], the input size *I* is effectively 29, and thus *D = (29 - 5) / 2 + 1 = 13*.</span></span> <span data-ttu-id="a39da-275">Men när **utfyllnad** = true, i stort sett *jag* hämtar ökar av *K - 1*; därför *D = ((28 + 4) - 5) / 27 + 1 = 2 / 2 + 1 = 13 + 1 = 14*.</span><span class="sxs-lookup"><span data-stu-id="a39da-275">However, when **Padding** = true, essentially *I* gets bumped up by *K - 1*; hence *D = ((28 + 4) - 5) / 2 + 1 = 27 / 2 + 1 = 13 + 1 = 14*.</span></span> <span data-ttu-id="a39da-276">Genom att ange värden för **UpperPad** och **LowerPad** du får mycket mer kontroll över utfyllnaden än om du bara ange **utfyllnad** = true.</span><span class="sxs-lookup"><span data-stu-id="a39da-276">By specifying values for **UpperPad** and **LowerPad** you get much more control over the padding than if you just set **Padding** = true.</span></span>

<span data-ttu-id="a39da-277">Mer information om convolutional nätverk och deras program finns i de här artiklarna:</span><span class="sxs-lookup"><span data-stu-id="a39da-277">For more information about convolutional networks and their applications, see these articles:</span></span>  

* [<span data-ttu-id="a39da-278">http://deeplearning.NET/Tutorial/lenet.HTML</span><span class="sxs-lookup"><span data-stu-id="a39da-278">http://deeplearning.net/tutorial/lenet.html </span></span>](http://deeplearning.net/tutorial/lenet.html)
* [<span data-ttu-id="a39da-279">http://Research.microsoft.com/pubs/68920/icdar03.PDF</span><span class="sxs-lookup"><span data-stu-id="a39da-279">http://research.microsoft.com/pubs/68920/icdar03.pdf</span></span>](http://research.microsoft.com/pubs/68920/icdar03.pdf) 
* [<span data-ttu-id="a39da-280">http://People.csail.MIT.edu/jvb/Papers/cnn_tutorial.PDF</span><span class="sxs-lookup"><span data-stu-id="a39da-280">http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf</span></span>](http://people.csail.mit.edu/jvb/papers/cnn_tutorial.pdf)  

## <a name="pooling-bundles"></a><span data-ttu-id="a39da-281">Poolning paket</span><span class="sxs-lookup"><span data-stu-id="a39da-281">Pooling bundles</span></span>
<span data-ttu-id="a39da-282">En **poolning paket** gäller geometri liknar convolutional anslutning, men den använder fördefinierade funktioner till noden källvärden för att härleda mål nodvärde.</span><span class="sxs-lookup"><span data-stu-id="a39da-282">A **pooling bundle** applies geometry similar to convolutional connectivity, but it uses predefined functions to source node values to derive the destination node value.</span></span> <span data-ttu-id="a39da-283">Därför har anslutningspoolen paket inga trainable tillstånd (vikterna eller eventuella fördomar).</span><span class="sxs-lookup"><span data-stu-id="a39da-283">Hence, pooling bundles have no trainable state (weights or biases).</span></span> <span data-ttu-id="a39da-284">Anslutningspooler paket stöder alla convolutional attribut utom **delning**, **MapCount**, och **vikterna**.</span><span class="sxs-lookup"><span data-stu-id="a39da-284">Pooling bundles support all the convolutional attributes except **Sharing**, **MapCount**, and **Weights**.</span></span>  

<span data-ttu-id="a39da-285">Normalt kärnor sammanfattning av intilliggande anslutningspoolen enheter inte överlappar varandra.</span><span class="sxs-lookup"><span data-stu-id="a39da-285">Typically, the kernels summarized by adjacent pooling units do not overlap.</span></span> <span data-ttu-id="a39da-286">Om Stride [d] är lika med KernelShape [d] i varje dimension är lagret fick traditionella lokala anslutningspoolen lager, som ofta används i convolutional neurala nätverk.</span><span class="sxs-lookup"><span data-stu-id="a39da-286">If Stride[d] is equal to KernelShape[d] in each dimension, the layer obtained is the traditional local pooling layer, which is commonly employed in convolutional neural networks.</span></span> <span data-ttu-id="a39da-287">Varje målnoden beräknar maximalt eller medelvärdet av aktiviteter för dess kernel i käll-lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-287">Each destination node computes the maximum or the mean of the activities of its kernel in the source layer.</span></span>  

<span data-ttu-id="a39da-288">I följande exempel visas ett anslutningspoolen paket:</span><span class="sxs-lookup"><span data-stu-id="a39da-288">The following example illustrates a pooling bundle:</span></span> 

    hidden P1 [5, 12, 12]
      from C1 max pool {
        InputShape  = [ 5, 24, 24];
        KernelShape = [ 1,  2,  2];
        Stride      = [ 1,  2,  2];
      }  

* <span data-ttu-id="a39da-289">Ariteten för paketet är 3 (längden på tuppeln **InputShape**, **KernelShape**, och **Stride**).</span><span class="sxs-lookup"><span data-stu-id="a39da-289">The arity of the bundle is 3 (the length of the tuples **InputShape**, **KernelShape**, and **Stride**).</span></span> 
* <span data-ttu-id="a39da-290">Antalet noder i käll-lagret är *5 * 24 * 24 = 2 880*.</span><span class="sxs-lookup"><span data-stu-id="a39da-290">The number of nodes in the source layer is *5 * 24 * 24 = 2880*.</span></span> 
* <span data-ttu-id="a39da-291">Detta är ett vanligt lokala anslutningspoolen lager eftersom **KernelShape** och **Stride** är lika.</span><span class="sxs-lookup"><span data-stu-id="a39da-291">This is a traditional local pooling layer because **KernelShape** and **Stride** are equal.</span></span> 
* <span data-ttu-id="a39da-292">Antalet noder i mållagret är *5 * 12 * 12 = 1440*.</span><span class="sxs-lookup"><span data-stu-id="a39da-292">The number of nodes in the destination layer is *5 * 12 * 12 = 1440*.</span></span>  

<span data-ttu-id="a39da-293">Mer information om anslutningspoolen lager finns dessa artiklar:</span><span class="sxs-lookup"><span data-stu-id="a39da-293">For more information about pooling layers, see these articles:</span></span>  

* <span data-ttu-id="a39da-294">[http://www.CS.Toronto.edu/~hinton/absps/imagenet.PDF](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (punkt 3.4)</span><span class="sxs-lookup"><span data-stu-id="a39da-294">[http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf) (Section 3.4)</span></span>
* [<span data-ttu-id="a39da-295">http://CS.nyu.edu/~koray/publis/lecun-iscas-10.PDF</span><span class="sxs-lookup"><span data-stu-id="a39da-295">http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf</span></span>](http://cs.nyu.edu/~koray/publis/lecun-iscas-10.pdf) 
* [<span data-ttu-id="a39da-296">http://CS.nyu.edu/~koray/publis/jarrett-iccv-09.PDF</span><span class="sxs-lookup"><span data-stu-id="a39da-296">http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf</span></span>](http://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf)

## <a name="response-normalization-bundles"></a><span data-ttu-id="a39da-297">Svaret normalisering paket</span><span class="sxs-lookup"><span data-stu-id="a39da-297">Response normalization bundles</span></span>
<span data-ttu-id="a39da-298">**Svaret normalisering** är en lokal normalisering schema som introducerades av Geoffrey Hinton et al i dokumentet [ImageNet Classiﬁcation med djupa Neurala nätverk för Convolutional](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf).</span><span class="sxs-lookup"><span data-stu-id="a39da-298">**Response normalization** is a local normalization scheme that was first introduced by Geoffrey Hinton, et al, in the paper [ImageNet Classiﬁcation with Deep Convolutional Neural Networks](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf).</span></span> <span data-ttu-id="a39da-299">Svaret normalisering används för att underlätta generalisering i neural nät.</span><span class="sxs-lookup"><span data-stu-id="a39da-299">Response normalization is used to aid generalization in neural nets.</span></span> <span data-ttu-id="a39da-300">När en neuron startar på en aktivering av mycket hög nivå, förhindrar ett lager för lokala svar normalisering aktivering andelen omgivande neurons.</span><span class="sxs-lookup"><span data-stu-id="a39da-300">When one neuron is firing at a very high activation level, a local response normalization layer suppresses the activation level of the surrounding neurons.</span></span> <span data-ttu-id="a39da-301">Detta görs med hjälp av tre parametrar (***α***, ***β***, och ***k***) och en convolutional struktur (eller nätverket form).</span><span class="sxs-lookup"><span data-stu-id="a39da-301">This is done by using three parameters (***α***, ***β***, and ***k***) and a convolutional structure (or neighborhood shape).</span></span> <span data-ttu-id="a39da-302">Varje neuron i mållagret ***y*** motsvarar en neuron ***x*** i käll-lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-302">Every neuron in the destination layer ***y*** corresponds to a neuron ***x*** in the source layer.</span></span> <span data-ttu-id="a39da-303">Aktivering andelen ***y*** ges genom följande formel, där ***f*** är den aktivering i en neuron och ***Nx*** är kärnan (eller som innehåller neurons i den nätverket av ***x***), enligt följande convolutional struktur:</span><span class="sxs-lookup"><span data-stu-id="a39da-303">The activation level of ***y*** is given by the following formula, where ***f*** is the activation level of a neuron, and ***Nx*** is the kernel (or the set that contains the neurons in the neighborhood of ***x***), as defined by the following convolutional structure:</span></span>  

![][1]  

<span data-ttu-id="a39da-304">Svaret normalisering paket stöder alla convolutional attribut utom **delning**, **MapCount**, och **vikterna**.</span><span class="sxs-lookup"><span data-stu-id="a39da-304">Response normalization bundles support all the convolutional attributes except **Sharing**, **MapCount**, and **Weights**.</span></span>  

* <span data-ttu-id="a39da-305">Om kernel innehåller neurons i samma karta som ***x***, normalisering schemat kallas **samma mappa normalisering**.</span><span class="sxs-lookup"><span data-stu-id="a39da-305">If the kernel contains neurons in the same map as ***x***, the normalization scheme is referred to as **same map normalization**.</span></span> <span data-ttu-id="a39da-306">Definiera samma karta normalisering första koordinaten i **InputShape** måste ha värdet 1.</span><span class="sxs-lookup"><span data-stu-id="a39da-306">To define same map normalization, the first coordinate in **InputShape** must have the value 1.</span></span>
* <span data-ttu-id="a39da-307">Om kernel innehåller neurons i samma spatial position som ***x***, men neurons finns i andra mappar, normalisering schemat kallas **tvärs över mappar normalisering**.</span><span class="sxs-lookup"><span data-stu-id="a39da-307">If the kernel contains neurons in the same spatial position as ***x***, but the neurons are in other maps, the normalization scheme is called **across maps normalization**.</span></span> <span data-ttu-id="a39da-308">Den här typen av svar normalisering implementerar en form av laterala inhibition inspirerat av typ av hittades i verkliga neurons skapar konkurrens om stora aktivering nivåer bland neuron utdata beräknad på olika maps.</span><span class="sxs-lookup"><span data-stu-id="a39da-308">This type of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activation levels amongst neuron outputs computed on different maps.</span></span> <span data-ttu-id="a39da-309">Om du vill definiera över maps normalisering första koordinaten måste vara ett heltal större än ett och inte större än antalet maps och resten av koordinaterna måste ha värdet 1.</span><span class="sxs-lookup"><span data-stu-id="a39da-309">To define across maps normalization, the first coordinate must be an integer greater than one and no greater than the number of maps, and the rest of the coordinates must have the value 1.</span></span>  

<span data-ttu-id="a39da-310">Eftersom svaret normalisering paket gäller en fördefinierad funktion källvärden nod att fastställa värdet för mål-nod, har de inget trainable tillstånd (vikt eller eventuella fördomar).</span><span class="sxs-lookup"><span data-stu-id="a39da-310">Because response normalization bundles apply a predefined function to source node values to determine the destination node value, they have no trainable state (weights or biases).</span></span>   

<span data-ttu-id="a39da-311">**Varning**: noder i mållagret motsvarar neurons som är centrala noder i kärnor.</span><span class="sxs-lookup"><span data-stu-id="a39da-311">**Alert**: The nodes in the destination layer correspond to neurons that are the central nodes of the kernels.</span></span> <span data-ttu-id="a39da-312">Om KernelShape [d] är udda, till exempel *KernelShape [d] / 2* motsvarar den centrala kernel-noden.</span><span class="sxs-lookup"><span data-stu-id="a39da-312">For example, if KernelShape[d] is odd, then *KernelShape[d]/2* corresponds to the central kernel node.</span></span> <span data-ttu-id="a39da-313">Om *KernelShape [d]* är jämnt och den centrala noden på *KernelShape [d] / 2-1*.</span><span class="sxs-lookup"><span data-stu-id="a39da-313">If *KernelShape[d]* is even, the central node is at *KernelShape[d]/2 - 1*.</span></span> <span data-ttu-id="a39da-314">Därför om **utfyllnad**[d] är False, först och senaste *KernelShape [d] / 2* noder har inte motsvarande noder i mållagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-314">Therefore, if **Padding**[d] is False, the first and the last *KernelShape[d]/2* nodes do not have corresponding nodes in the destination layer.</span></span> <span data-ttu-id="a39da-315">Om du vill undvika detta kan du definiera **utfyllnad** som [true, true,..., true].</span><span class="sxs-lookup"><span data-stu-id="a39da-315">To avoid this situation, define **Padding** as [true, true, …, true].</span></span>  

<span data-ttu-id="a39da-316">Utöver de fyra attributen som beskrivs ovan, stöd svar normalisering paket också för följande attribut:</span><span class="sxs-lookup"><span data-stu-id="a39da-316">In addition to the four attributes described earlier, response normalization bundles also support the following attributes:</span></span>  

* <span data-ttu-id="a39da-317">**Alpha**: (obligatoriskt) anger ett värde som motsvarar ***α*** i föregående formel.</span><span class="sxs-lookup"><span data-stu-id="a39da-317">**Alpha**: (required) Specifies a floating-point value that corresponds to ***α*** in the previous formula.</span></span> 
* <span data-ttu-id="a39da-318">**Beta**: (obligatoriskt) anger ett värde som motsvarar ***β*** i föregående formel.</span><span class="sxs-lookup"><span data-stu-id="a39da-318">**Beta**: (required) Specifies a floating-point value that corresponds to ***β*** in the previous formula.</span></span> 
* <span data-ttu-id="a39da-319">**Förskjutning**: (valfritt) anger ett värde som motsvarar ***k*** i föregående formel.</span><span class="sxs-lookup"><span data-stu-id="a39da-319">**Offset**: (optional) Specifies a floating-point value that corresponds to ***k*** in the previous formula.</span></span> <span data-ttu-id="a39da-320">Standardvärdet 1.</span><span class="sxs-lookup"><span data-stu-id="a39da-320">It defaults to 1.</span></span>  

<span data-ttu-id="a39da-321">I följande exempel definierar ett svar normalisering paket med hjälp av dessa attribut:</span><span class="sxs-lookup"><span data-stu-id="a39da-321">The following example defines a response normalization bundle using these attributes:</span></span>  

    hidden RN1 [5, 10, 10]
      from P1 response norm {
        InputShape  = [ 5, 12, 12];
        KernelShape = [ 1,  3,  3];
        Alpha = 0.001;
        Beta = 0.75;
      }  

* <span data-ttu-id="a39da-322">Käll-lagret innehåller fem kartor, med aof dimension 12 x 12 summering 1440 noder.</span><span class="sxs-lookup"><span data-stu-id="a39da-322">The source layer includes five maps, each with aof dimension of 12x12, totaling in 1440 nodes.</span></span> 
* <span data-ttu-id="a39da-323">Värdet för **KernelShape** anger att detta är en samma normalisering kartskiktet, där nätverket är en rektangel 3 x 3.</span><span class="sxs-lookup"><span data-stu-id="a39da-323">The value of **KernelShape** indicates that this is a same map normalization layer, where the neighborhood is a 3x3 rectangle.</span></span> 
* <span data-ttu-id="a39da-324">Standardvärdet för **utfyllnad** är False, vilket mållagret har bara 10 noder i varje dimension.</span><span class="sxs-lookup"><span data-stu-id="a39da-324">The default value of **Padding** is False, thus the destination layer has only 10 nodes in each dimension.</span></span> <span data-ttu-id="a39da-325">Om du vill inkludera en nod i mållagret som motsvarar varje nod i käll-lagret, lägger du till utfyllnad = [true, true, true]; och ändra storlek på RN1 [5, 12, 12].</span><span class="sxs-lookup"><span data-stu-id="a39da-325">To include one node in the destination layer that corresponds to every node in the source layer, add Padding = [true, true, true]; and change the size of RN1 to [5, 12, 12].</span></span>  

## <a name="share-declaration"></a><span data-ttu-id="a39da-326">Dela förklaring</span><span class="sxs-lookup"><span data-stu-id="a39da-326">Share declaration</span></span>
<span data-ttu-id="a39da-327">NET # stöder definiera flera paket med delade vikter.</span><span class="sxs-lookup"><span data-stu-id="a39da-327">Net# optionally supports defining multiple bundles with shared weights.</span></span> <span data-ttu-id="a39da-328">Vikten av alla två paket kan delas om deras strukturer är identiska.</span><span class="sxs-lookup"><span data-stu-id="a39da-328">The weights of any two bundles can be shared if their structures are the same.</span></span> <span data-ttu-id="a39da-329">Följande syntax definierar paket med delade vikter:</span><span class="sxs-lookup"><span data-stu-id="a39da-329">The following syntax defines bundles with shared weights:</span></span>  

    share-declaration:
        share    {    layer-list    }
        share    {    bundle-list    }
       share    {    bias-list    }

    layer-list:
        layer-name    ,    layer-name
        layer-list    ,    layer-name

    bundle-list:
       bundle-spec    ,    bundle-spec
        bundle-list    ,    bundle-spec

    bundle-spec:
       layer-name    =>     layer-name

    bias-list:
        bias-spec    ,    bias-spec
        bias-list    ,    bias-spec

    bias-spec:
        1    =>    layer-name

    layer-name:
        identifier  

<span data-ttu-id="a39da-330">Till exempel anger följande resurs-deklaration lagernamn, som anger att både vikterna och eventuella fördomar ska delas:</span><span class="sxs-lookup"><span data-stu-id="a39da-330">For example, the following share-declaration specifies the layer names, indicating that both weights and biases should be shared:</span></span>  

    Const {
      InputSize = 37;
      HiddenSize = 50;
    }
    input {
      Data1 [InputSize];
      Data2 [InputSize];
    }
    hidden {
      H1 [HiddenSize] from Data1 all;
      H2 [HiddenSize] from Data2 all;
    }
    output Result [2] {
      from H1 all;
      from H2 all;
    }
    share { H1, H2 } // share both weights and biases  

* <span data-ttu-id="a39da-331">Funktionerna för inkommande delas upp i två lika storlek inkommande lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-331">The input features are partitioned into two equal sized input layers.</span></span> 
* <span data-ttu-id="a39da-332">Dolda lager för att beräkna högre nivå funktioner på två inkommande lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-332">The hidden layers then compute higher level features on the two input layers.</span></span> 
* <span data-ttu-id="a39da-333">Resurs-deklarationen anger att *H1* och *H2* måste beräknas på samma sätt från deras respektive indata.</span><span class="sxs-lookup"><span data-stu-id="a39da-333">The share-declaration specifies that *H1* and *H2* must be computed in the same way from their respective inputs.</span></span>  

<span data-ttu-id="a39da-334">Du kan också anges detta med två olika resurs-deklarationer enligt följande:</span><span class="sxs-lookup"><span data-stu-id="a39da-334">Alternatively, this could be specified with two separate share-declarations as follows:</span></span>  

    share { Data1 => H1, Data2 => H2 } // share weights  

<!-- -->

    share { 1 => H1, 1 => H2 } // share biases  

<span data-ttu-id="a39da-335">Du kan använda kort form endast när lagren innehåller ett paket.</span><span class="sxs-lookup"><span data-stu-id="a39da-335">You can use the short form only when the layers contain a single bundle.</span></span> <span data-ttu-id="a39da-336">I allmänhet är delning möjligt endast när den aktuella strukturen är identiska, vilket innebär att de har samma storlek, samma convolutional geometri och så vidare.</span><span class="sxs-lookup"><span data-stu-id="a39da-336">In general, sharing is possible only when the relevant structure is identical, meaning that they have the same size, same convolutional geometry, and so forth.</span></span>  

## <a name="examples-of-net-usage"></a><span data-ttu-id="a39da-337">Exempel på användning av Net #</span><span class="sxs-lookup"><span data-stu-id="a39da-337">Examples of Net# usage</span></span>
<span data-ttu-id="a39da-338">Det här avsnittet innehåller några exempel på hur du kan använda Net # definiera hur dolda lager interagera med andra lager, och skapa convolutional nätverk om du vill lägga till dolda lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-338">This section provides some examples of how you can use Net# to add hidden layers, define the way that hidden layers interact with other layers, and build convolutional networks.</span></span>   

### <a name="define-a-simple-custom-neural-network-hello-world-example"></a><span data-ttu-id="a39da-339">Definiera en enkel anpassade neurala nätverket: ”Hello World”-exempel</span><span class="sxs-lookup"><span data-stu-id="a39da-339">Define a simple custom neural network: "Hello World" example</span></span>
<span data-ttu-id="a39da-340">Det här enkla exemplet visar hur du skapar en neurala nätverket modell som har ett dolt lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-340">This simple example demonstrates how to create a neural network model that has a single hidden layer.</span></span>  

    input Data auto;
    hidden H [200] from Data all;
    output Out [10] sigmoid from H all;  

<span data-ttu-id="a39da-341">I exemplet vissa grundläggande kommandon på följande sätt:</span><span class="sxs-lookup"><span data-stu-id="a39da-341">The example illustrates some basic commands as follows:</span></span>  

* <span data-ttu-id="a39da-342">Den första raden definierar inkommande lagret (med namnet *Data*).</span><span class="sxs-lookup"><span data-stu-id="a39da-342">The first line defines the input layer (named *Data*).</span></span> <span data-ttu-id="a39da-343">När du använder den **automatisk** nyckelord, det neurala nätverket inkluderar automatiskt alla kolumner i funktionen i inkommande exemplen.</span><span class="sxs-lookup"><span data-stu-id="a39da-343">When you use the  **auto** keyword, the neural network automatically includes all feature columns in the input examples.</span></span> 
* <span data-ttu-id="a39da-344">Den andra raden skapar det dolda lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-344">The second line creates the hidden layer.</span></span> <span data-ttu-id="a39da-345">Namnet *H* tilldelas det dolda lagret som har 200 noder.</span><span class="sxs-lookup"><span data-stu-id="a39da-345">The name *H* is assigned to the hidden layer, which has 200 nodes.</span></span> <span data-ttu-id="a39da-346">Det här lagret är fullständigt ansluten till inkommande lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-346">This layer is fully connected to the input layer.</span></span>
* <span data-ttu-id="a39da-347">Den tredje raden definierar utdata lagret (med namnet *O*), som innehåller 10 utdata-noder.</span><span class="sxs-lookup"><span data-stu-id="a39da-347">The third line defines the output layer (named *O*), which contains 10 output nodes.</span></span> <span data-ttu-id="a39da-348">Om det neurala nätverket används för klassificering, är det en utdata nod per klass.</span><span class="sxs-lookup"><span data-stu-id="a39da-348">If the neural network is used for classification, there is one output node per class.</span></span> <span data-ttu-id="a39da-349">Nyckelordet **sigmoid** anger att funktionen utdata ska användas i output-lagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-349">The keyword **sigmoid** indicates that the output function is applied to the output layer.</span></span>   

### <a name="define-multiple-hidden-layers-computer-vision-example"></a><span data-ttu-id="a39da-350">Definiera flera dolda lager: datorn vision exempel</span><span class="sxs-lookup"><span data-stu-id="a39da-350">Define multiple hidden layers: computer vision example</span></span>
<span data-ttu-id="a39da-351">Exemplet nedan visar hur du definierar ett lite mer komplext neurala nätverk med flera anpassade dolda lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-351">The following example demonstrates how to define a slightly more complex neural network, with multiple custom hidden layers.</span></span>  

    // Define the input layers 
    input Pixels [10, 20];
    input MetaData [7];

    // Define the first two hidden layers, using data only from the Pixels input
    hidden ByRow [10, 12] from Pixels where (s,d) => s[0] == d[0];
    hidden ByCol [5, 20] from Pixels where (s,d) => abs(s[1] - d[1]) <= 1;

    // Define the third hidden layer, which uses as source the hidden layers ByRow and ByCol
    hidden Gather [100] 
    {
      from ByRow all;
      from ByCol all;
    }

    // Define the output layer and its sources
    output Result [10]  
    {
      from Gather all;
      from MetaData all;
    }  

<span data-ttu-id="a39da-352">Det här exemplet visar flera funktioner från språket som specifikationen neurala nätverk:</span><span class="sxs-lookup"><span data-stu-id="a39da-352">This example illustrates several features of the neural networks specification language:</span></span>  

* <span data-ttu-id="a39da-353">Strukturen har två inkommande lager *bildpunkter* och *MetaData*.</span><span class="sxs-lookup"><span data-stu-id="a39da-353">The structure has two input layers, *Pixels* and *MetaData*.</span></span>
* <span data-ttu-id="a39da-354">Den *bildpunkter* layer är ett skikt som källa för två anslutning paket, med målet lager *ByRow* och *ByCol*.</span><span class="sxs-lookup"><span data-stu-id="a39da-354">The *Pixels* layer is a source layer for two connection bundles, with destination layers, *ByRow* and *ByCol*.</span></span>
* <span data-ttu-id="a39da-355">Lagren *samla in* och *resultatet* är målet lager i flera paket för anslutningen.</span><span class="sxs-lookup"><span data-stu-id="a39da-355">The layers *Gather* and *Result* are destination layers in multiple connection bundles.</span></span>
* <span data-ttu-id="a39da-356">Utdata-lagret *resultatet*, är en mållagret i två anslutning paket; en med andra nivån dolda (samla) som en mållagret och andra med det inkommande lagret (MetaData) som en mållagret.</span><span class="sxs-lookup"><span data-stu-id="a39da-356">The output layer, *Result*, is a destination layer in two connection bundles; one with the second level hidden (Gather) as a destination layer, and the other with the input layer (MetaData) as a destination layer.</span></span>
* <span data-ttu-id="a39da-357">Dolda lager *ByRow* och *ByCol*, ange filtrerad anslutning med hjälp av predikatuttryck.</span><span class="sxs-lookup"><span data-stu-id="a39da-357">The hidden layers, *ByRow* and *ByCol*, specify filtered connectivity by using predicate expressions.</span></span> <span data-ttu-id="a39da-358">Mer exakt noden i *ByRow* på [x, y] är ansluten till noderna i *bildpunkter* att har det första indexet samordna lika med noden har första samordna x.</span><span class="sxs-lookup"><span data-stu-id="a39da-358">More precisely, the node in *ByRow* at [x, y] is connected to the nodes in *Pixels* that have the first index coordinate equal to the node's first coordinate, x.</span></span> <span data-ttu-id="a39da-359">På liknande sätt noden i *ByCol på [x, y] är ansluten till noderna i _Pixels* att ha andra indexet samordna inom någon av noden har andra samordna y.</span><span class="sxs-lookup"><span data-stu-id="a39da-359">Similarly, the node in *ByCol at [x, y] is connected to the nodes in _Pixels* that have the second index coordinate within one of the node's second coordinate, y.</span></span>  

### <a name="define-a-convolutional-network-for-multiclass-classification-digit-recognition-example"></a><span data-ttu-id="a39da-360">Ange ett convolutional nätverk för multiklass-baserad klassificering: siffra recognition exempel</span><span class="sxs-lookup"><span data-stu-id="a39da-360">Define a convolutional network for multiclass classification: digit recognition example</span></span>
<span data-ttu-id="a39da-361">Definitionen av följande nätverk är utformad för att identifiera siffror och det visar att vissa avancerade tekniker för att anpassa en neurala nätverket.</span><span class="sxs-lookup"><span data-stu-id="a39da-361">The definition of the following network is designed to recognize numbers, and it illustrates some advanced techniques for customizing a neural network.</span></span>  

    input Image [29, 29];
    hidden Conv1 [5, 13, 13] from Image convolve 
    {
       InputShape  = [29, 29];
       KernelShape = [ 5,  5];
       Stride      = [ 2,  2];
       MapCount    = 5;
    }
    hidden Conv2 [50, 5, 5]
    from Conv1 convolve 
    {
       InputShape  = [ 5, 13, 13];
       KernelShape = [ 1,  5,  5];
       Stride      = [ 1,  2,  2];
       Sharing     = [false, true, true];
       MapCount    = 10;
    }
    hidden Hid3 [100] from Conv2 all;
    output Digit [10] from Hid3 all;  


* <span data-ttu-id="a39da-362">Strukturen har ett enda inkommande lager *bild*.</span><span class="sxs-lookup"><span data-stu-id="a39da-362">The structure has a single input layer, *Image*.</span></span>
* <span data-ttu-id="a39da-363">Nyckelordet **convolve** betyder att lagren heter *Conv1* och *Conv2* är convolutional lager.</span><span class="sxs-lookup"><span data-stu-id="a39da-363">The keyword **convolve** indicates that the layers named *Conv1* and *Conv2* are convolutional layers.</span></span> <span data-ttu-id="a39da-364">Var och en av dessa lager-deklarationer följs av en lista över Faltning-attribut.</span><span class="sxs-lookup"><span data-stu-id="a39da-364">Each of these layer declarations is followed by a list of the convolution attributes.</span></span>
* <span data-ttu-id="a39da-365">Net har en tredje dolda lagret, *Hid3*, som är anslutet till det andra dolda lagret *Conv2*.</span><span class="sxs-lookup"><span data-stu-id="a39da-365">The net has a third hidden layer, *Hid3*, which is fully connected to the second hidden layer, *Conv2*.</span></span>
* <span data-ttu-id="a39da-366">Utdata-lagret *siffra*, ansluts bara till det tredje dolda lagret *Hid3*.</span><span class="sxs-lookup"><span data-stu-id="a39da-366">The output layer, *Digit*, is connected only to the third hidden layer, *Hid3*.</span></span> <span data-ttu-id="a39da-367">Nyckelordet **alla** anger att utdata lagret fullständigt är ansluten till *Hid3*.</span><span class="sxs-lookup"><span data-stu-id="a39da-367">The keyword **all** indicates that the output layer is fully connected to *Hid3*.</span></span>
* <span data-ttu-id="a39da-368">Ariteten för Faltning är tre (längden på tuppeln **InputShape**, **KernelShape**, **Stride**, och **delning**).</span><span class="sxs-lookup"><span data-stu-id="a39da-368">The arity of the convolution is three (the length of the tuples **InputShape**, **KernelShape**, **Stride**, and **Sharing**).</span></span> 
* <span data-ttu-id="a39da-369">Antalet vikter per kernel är *1 + **KernelShape**\[0] * **KernelShape**\[1] * **KernelShape** \[ 2] = 1 + 1 * 5 * 5 = 26. Eller 26 * 50 = 1300*.</span><span class="sxs-lookup"><span data-stu-id="a39da-369">The number of weights per kernel is *1 + **KernelShape**\[0] * **KernelShape**\[1] * **KernelShape**\[2] = 1 + 1 * 5 * 5 = 26. Or 26 * 50 = 1300*.</span></span>
* <span data-ttu-id="a39da-370">Du kan beräkna noder i varje dolda lagret på följande sätt:</span><span class="sxs-lookup"><span data-stu-id="a39da-370">You can calculate the nodes in each hidden layer as follows:</span></span>
  * <span data-ttu-id="a39da-371">**NodeCount**\[0] = (5 - 1) / 1 + 1 = 5.</span><span class="sxs-lookup"><span data-stu-id="a39da-371">**NodeCount**\[0] = (5 - 1) / 1 + 1 = 5.</span></span>
  * <span data-ttu-id="a39da-372">**NodeCount**\[1] = (13-5) / 2 + 1 = 5.</span><span class="sxs-lookup"><span data-stu-id="a39da-372">**NodeCount**\[1] = (13 - 5) / 2 + 1 = 5.</span></span> 
  * <span data-ttu-id="a39da-373">**NodeCount**\[2] = (13-5) / 2 + 1 = 5.</span><span class="sxs-lookup"><span data-stu-id="a39da-373">**NodeCount**\[2] = (13 - 5) / 2 + 1 = 5.</span></span> 
* <span data-ttu-id="a39da-374">Det totala antalet noder kan beräknas med hjälp av deklarerade dimensionaliteten för lagret [50, 5, 5], enligt följande:  ***MapCount** * **NodeCount**\[0] * **NodeCount**\[1] * **NodeCount**\[2] = 10 * 5 * 5 * 5*</span><span class="sxs-lookup"><span data-stu-id="a39da-374">The total number of nodes can be calculated by using the declared dimensionality of the layer, [50, 5, 5], as follows: ***MapCount** * **NodeCount**\[0] * **NodeCount**\[1] * **NodeCount**\[2] = 10 * 5 * 5 * 5*</span></span>
* <span data-ttu-id="a39da-375">Eftersom **delning**[d] har värdet False för *d == 0*, antalet kärnor är  ***MapCount** * **NodeCount** \[0] = 10 * 5 = 50*.</span><span class="sxs-lookup"><span data-stu-id="a39da-375">Because **Sharing**[d] is False only for *d == 0*, the number of kernels is ***MapCount** * **NodeCount**\[0] = 10 * 5 = 50*.</span></span> 

## <a name="acknowledgements"></a><span data-ttu-id="a39da-376">Bekräftelser</span><span class="sxs-lookup"><span data-stu-id="a39da-376">Acknowledgements</span></span>
<span data-ttu-id="a39da-377">Net #-språk för att anpassa arkitekturen för neurala nätverk har utvecklats på Microsoft av Shon Katzenberger (systemarkitekt, Machine Learning) och Alexey Kamenev (programvara tekniker, Microsoft Research).</span><span class="sxs-lookup"><span data-stu-id="a39da-377">The Net# language for customizing the architecture of neural networks was developed at Microsoft by Shon Katzenberger (Architect, Machine Learning) and Alexey Kamenev (Software Engineer, Microsoft Research).</span></span> <span data-ttu-id="a39da-378">Det används internt för maskininlärning projekt och program, från identifiering av avbildningen till text analytics.</span><span class="sxs-lookup"><span data-stu-id="a39da-378">It is used internally for machine learning projects and applications ranging from image detection to text analytics.</span></span> <span data-ttu-id="a39da-379">Mer information finns i [Neural nät i Azure ML - introduktion till Net #](http://blogs.technet.com/b/machinelearning/archive/2015/02/16/neural-nets-in-azure-ml-introduction-to-net.aspx)</span><span class="sxs-lookup"><span data-stu-id="a39da-379">For more information, see [Neural Nets in Azure ML - Introduction to Net#](http://blogs.technet.com/b/machinelearning/archive/2015/02/16/neural-nets-in-azure-ml-introduction-to-net.aspx)</span></span>

<span data-ttu-id="a39da-380">[1]:./media/machine-learning-azure-ml-netsharp-reference-guide/formula_large.gif</span><span class="sxs-lookup"><span data-stu-id="a39da-380">[1]:./media/machine-learning-azure-ml-netsharp-reference-guide/formula_large.gif</span></span>

